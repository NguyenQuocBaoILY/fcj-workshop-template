[{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS Workshop 1. Event Information Event Name: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS Workshop Time: 8:30 – 12:00 Organizer: Amazon Web Services (AWS) 2. Purpose of Participation I attended the event to gain a deeper understanding of the AI/ML/GenAI ecosystem on AWS, learn the end-to-end ML lifecycle with Amazon SageMaker, explore how Foundation Models operate on Amazon Bedrock, and practice Prompt Engineering, RAG, and Agents.\nAdditionally, I aimed to observe how a real GenAI chatbot is built in a live environment.\n3. Agenda Summary 8:30 – 9:00 — Check-in \u0026amp; Introduction Main activities:\nParticipant check-in and networking Introduction to workshop objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam Key insight:\nGained an updated view of AI/ML adoption in Vietnamese enterprises and how AWS supports digital transformation through ML and GenAI.\n9:00 – 10:30 — AWS AI/ML Services Overview Main content:\nIntroduction to Amazon SageMaker as an end-to-end ML platform Data preparation and labeling Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio Key insight:\nUnderstood the full ML lifecycle on AWS—from data preparation → training → tuning → deployment—with hands-on exposure to SageMaker Studio.\n10:30 – 10:45 — Coffee Break A short break for discussion and networking with AWS experts and participants.\n10:45 – 12:00 — Generative AI with Amazon Bedrock Main content:\nFoundation Models: Claude, Llama, Titan — differences and selection criteria Prompt Engineering: Chain-of-Thought, Few-shot learning RAG (Retrieval-Augmented Generation): Architecture and Knowledge Base integration Bedrock Agents: Multi-step workflows and tool integrations Guardrails: Safety mechanisms and content filtering Live Demo: Building a GenAI chatbot Key insight:\nLearned how to choose the right FM, apply prompt engineering techniques, build a complete RAG system, automate workflows with Agents, apply Guardrails, and follow the full GenAI chatbot deployment process.\nKey Takeaways Comprehensive understanding of ML and GenAI on AWS Clear differences and use cases of Foundation Models (Claude/Llama/Titan) Applied Chain-of-Thought and Few-shot methods to enhance model output Learned prompt design for complex workflows Understood the importance of RAG in enterprise applications Learned how to build Agents with tool integrations Hands-on experience with SageMaker Studio and end-to-end ML workflow Understood how to deploy enterprise-grade GenAI chatbots using AWS tools Practical Application Apply RAG to internal chatbots or document search systems Use SageMaker to train or fine-tune ML models Use Prompt Engineering to improve Foundation Model output quality Integrate Bedrock Agents to automate business workflows Build GenAI demos for team or project use Event Experience Attending “AI/ML/GenAI on AWS” was a highly valuable experience that strengthened my understanding of how enterprises design and deploy AI/ML and GenAI solutions in real-world environments.\nHighlights AWS Experts: Clear explanation of the AI/ML roadmap for Vietnam and practical demos of SageMaker and Bedrock Hands-on Sessions: Observed the full train → tune → deploy ML workflow and GenAI chatbot building Networking: Connected with AWS engineers and peers, learning from real GenAI case studies Lessons Learned: GenAI is not just a model—it is a full workflow (Prompt → RAG → Agents → Guardrails). SageMaker standardizes ML lifecycle, and choosing the right FM influences cost and efficiency. Important Lessons GenAI requires a complete workflow, not just prompt usage RAG is essential for enterprise applications using internal data Agents significantly expand automation capabilities SageMaker provides a scalable and structured ML development process FM selection dramatically affects latency, accuracy, and cost efficiency Event Photos "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguyen Quoc Bao\nPhone Number: 0909439694\nEmail: Baonqse183321@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attended the Kick-off AWS FCJ Workforce, gaining a better understanding of AWS infrastructure - Read and take note of internship unit rules and regulations 09/06/2025 09/08/2025 3 - Account Creation and Billing Setup + Create new AWS account + MFA for AWS Account 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/vi/3-create-admin-user-and-group// 4 - Learn about Identity and Access Management (IAM) - Practice: + Create admin group and user group 09/10/2025 09/10/2025 https://000001.awsstudygroup.com/vi/3-create-admin-user-and-group/ 5 - Learn about the budget - Practice: + Create Budget by Template + Create Cost Budget Tutorial + Creating a Usage Budget in AWS + Creating a Reservation Instance (RI) Budget 09/11/2025 09/11/2025 https://000007.awsstudygroup.com/vi/ 6 - Learn about AWS Support Packages (IAM) Practice: + Types of supports request + Change support package + Manage support requests 09/12/2025 09/12/2025 hhttps://000009.awsstudygroup.com/vi/ Week 1 Achievements: Project \u0026amp; Compliance: Attended the Kick-off, understood the project orientation and internship rules, and became acquainted with FCJ members. AWS Core Setup: Successfully created and securely configured the AWS Free Tier account. Security Foundation: Successfully set up MFA for the Root User and created IAM Admin/User Groups, applying basic security principles. Cost Management: Practiced creating AWS Budgets to monitor and control costs from the outset. Support \u0026amp; Service: Understood the different types of AWS Support Packages and learned how to manage support request "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learning and practicing AWS VPC, Networking, and Multi-VPC Connectivity\nWeek 3: Learning Amazon EC2, Backup, Storage Gateway, and S3 Fundamentals\nWeek 4: VM Import/Export and Amazon FSx for Windows File Server\nWeek 5: AWS Security, Tags, and IAM Permission Boundaries\nWeek 6: AWS Security \u0026amp; Data Analytics Fundamentals\nWeek 7: AWS Infrastructure \u0026amp; Project Planning\nWeek 8: Midterm Exam Preparation and AWS Cloud Practitioner Practice\nWeek 9: Project Development \u0026amp; AWS Security Overview\nWeek 10: Project and Web Interface Development\nWeek 11: Ongoing Project \u0026amp; Web Development\nWeek 12: Project Deployment on AWS\n"},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.8-cognito/5.8.1-create-user-pool/","title":"Create Cognito User Pool","tags":[],"description":"","content":"Create User Pool In this step, you will create a Cognito User Pool to manage user authentication for your application.\nNavigate to AWS Cognito service in the AWS Console Click Create user pool Configure options\nOptions for sign-in identifiers:\nSelect Email (allows users to sign in with email) Uncheck Phone number and Username Self-registration:\n✅ Enable self-registration (allows users to sign up themselves) Required attributes for sign-up:\nSelect email and name as required attributes Click Create user pool Enter User Pool Name\nUser pool name: TaskManagementUserPool Click Create user pool Verify User Pool Creation After successful creation, you should see your User Pool in the Cognito console with default configurations:\nReview Default Configuration Cognito automatically creates the User Pool with default settings:\nSign-in experience:\nSign-in options: Username (can be changed to Email) Password policy: Cognito defaults Multi-factor authentication: Optional Sign-up experience:\nSelf-service sign-up: Enabled Email verification: Enabled Required attributes: email Message delivery:\nEmail provider: Send email with Cognito App integration:\nHosted UI: Not configured (will be configured in later steps) Important Note ⚠️ Warning: Options for sign-in identifiers and required attributes cannot be changed after the User Pool is created. If you need to change these, you must create a new User Pool.\nNote down the User Pool ID from the overview page as you\u0026rsquo;ll need it for the next steps:\nIn the following steps, we will configure detailed features like password policies, email verification, and create App Clients.\n"},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Building the TaskHub Platform with the DevSecOps Model on AWS Workshop Introduction In this workshop, you will build the entire TaskHub platform following the AWS Serverless + DevSecOps model, based on real-world architectures widely adopted by modern enterprises to achieve scalability, security, and cost efficiency.\nThe workshop is divided into groups of AWS services, helping you:\nUnderstand the role of each service in a modern serverless architecture. Deploy API Gateway, Lambda, DynamoDB, Cognito, and S3/CloudFront hands-on. Apply DevSecOps practices using CodePipeline, CodeBuild, and CodeGuru. Implement security at both the Backend (KMS, Secrets Manager) and Edge (WAF, Shield). Fully integrate a Next.js frontend with an AWS serverless backend. Architectural Overview In this workshop, you will build all the key components of the TaskHub platform:\nAmazon S3 – Stores the static build of the Next.js application. Amazon CloudFront – Delivers the UI globally with low latency. AWS WAF \u0026amp; AWS Shield – Protect the application from DDoS attacks and OWASP Top 10 threats. Amazon Cognito – Handles user authentication, identity management, and Admin/Member role authorization. Amazon API Gateway – Serves as the entry point for frontend requests. AWS Lambda (Node.js/TypeScript) – Processes all business logic. Amazon DynamoDB – Stores tasks, users, and progress data. AWS KMS – Encrypts data stored in DynamoDB. AWS Secrets Manager – Stores sensitive information and API keys securely. AWS CodePipeline – Automates the entire CI/CD process. AWS CodeBuild – Builds frontend/backend and performs security scans. AWS CodeGuru Reviewer – Analyzes code quality and provides optimization recommendations. AWS CloudFormation – Deploys infrastructure via IaC. Amazon CloudWatch Logs – Captures logs from Lambda and API Gateway. AWS X-Ray – Provides system-wide tracing and latency analysis. Amazon SNS – Sends system events and alert notifications. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"How Patronus AI Helps Businesses Enhance Reliability When Using Generative AI by Aditya Shahani and Bonnie McClure | on MAY 02, 2024 | in Customer Solutions, Generative AI, Startup\nIn recent years, especially since the launch of ChatGPT in 2022, the transformative potential of Generative AI has become undeniable for organizations of all sizes and across many industries. The next wave of adoption has begun, as businesses are rushing to deploy Generative AI tools to increase efficiency and improve customer experiences. According to the 2023 McKinsey Report, Generative AI could contribute an additional $2.6 to $4.4 trillion to the global economy annually, increasing AI’s overall economic impact by roughly 15–40%. Meanwhile, the latest global CEO survey by IBM shows that 50% of respondents have already begun integrating Generative AI into their products and services.\nAs generative AI goes mainstream, however, customers and businesses are increasingly expressing concern about its trustworthiness and reliability. And it can be unclear why given inputs lead to certain outputs, making it difficult for companies to evaluate the results of their generative AI. Patronus AI, a company founded by machine learning (ML) experts Anand Kannappan and Rebecca Qian, has set out to tackle this problem. With its AI-driven automated evaluation and security platform, Patronus helps its customers use large language models (LLMs) confidently and responsibly while minimizing the risk of errors. The startup’s aim is to make AI models more trustworthy and more usable. “That’s become the big question in the past year. Every enterprise wants to use language models, but they’re concerned about the risks and even just the reliability of how they work, especially for their very specific use cases,” explains Anand. “Our mission is to boost enterprise confidence in generative AI.”\nReaping the benefits and managing the risks of generative AI Generative AI is a type of AI that uses ML to generate new data similar to the data it was trained on. By learning the patterns and structure of the input datasets, generative AI produces original content—images, text, and even lines of code. Generative AI applications are powered by ML models that have been pre-trained on vast amounts of data, most notably LLMs trained on trillions of words across a range of natural language tasks.\nThe potential business benefits are sky-high. Firms have shown interest in using LLMs to leverage their own internal data through retrieval, to produce memos and presentations, to improve automated chat assistance, and to auto-complete code generation in software development. Anand also points to the whole range of other use cases that have not yet been realized. “There’s a lot of different industries that generative AI hasn’t disrupted yet. We’re really just at the early innings of everything that we’re seeing so far.”\nAs organizations consider expanding their use of generative AI, the issue of trustworthiness becomes more pressing. Users want to ensure their outputs comply with company regulations and policies while avoiding unsafe or illegal outcomes. “For larger companies and enterprises, especially in regulated industries,” explains Anand, “there are a lot of mission-critical scenarios where they want to use generative AI, but they’re concerned that if a mistake happens, it puts their reputation at risk, or even their own customers at risk.”\nPatronus helps customers manage these risks and boost confidence in generative AI by improving the ability to measure, analyze, and experiment with the performance of the models in question. “It’s really about making sure that, regardless of the way that your system was developed, the overall testing and evaluation of everything is very robust and standardized,” says Anand. “And that’s really what’s missing right now: everyone wants to use language models, but there’s no really established or standardized framework of how to properly test them in a much more scientific way.”\nEnhancing trustworthiness and performance The automated Patronus platform allows customers to evaluate and compare the performance of different LLMs in real-world scenarios, thereby reducing the risk of undesired outputs. Patronus uses novel ML techniques to help customers automatically generate adversarial test suites and score and benchmark language model performance based on Patronus’s proprietary taxonomy of criteria. For example, the FinanceBench dataset is the industry’s first benchmark for LLM performance on financial questions.\n“Everything we do at Patronus is very focused around helping companies be able to catch language model mistakes in a much more scalable and automated way,” says Anand. Many large companies are currently spending vast amounts on internal quality assurance teams and external consultants, who manually create test cases and grade their LLM outputs in spreadsheets, but Patronus’s AI-driven approach saves the need for such a slow and expensive process.\n“Natural Language Processing (NLP) is quite empirical, so there is a lot of experimentation work that we are doing to ultimately figure out which evaluation techniques work the best,” explains Anand. “How can we enable those kinds of things in our product so that people can leverage the value … from the techniques that we figured out work the best, very easily and quickly? And how can they get performance improvements, not only for their own system, but even for the evaluation against that system that they’ve been able to do now because of Patronus?”\nWhat results is a virtuous cycle: the more a company uses the product and gives feedback via the thumbs or thumbs down feature, the better its evaluations become, and the better the company’s own systems become as a result.\nBoosting confidence through improved results and understandability To unlock the potential of generative AI, improving its reliability and trustworthiness is vital. Potential adopters across a variety of industries and use cases are regularly held back—not just by the fact that mistakes are sometimes made by AI applications—but also by the difficulty of understanding how or why a problem has occurred, and how to avoid that happening in the future.\n“What everyone is really asking for is a better way to have a lot more confidence in something when you roll it out to production,” says Anand. “And when you put it in front of your own employees, and even end customers, then that’s hundreds, thousands, or tens of thousands of people, so you want to make sure that those kinds of challenges are limited as much as possible. And, for the ones that do happen, you want to know when they happen and why.”\nOne of Patronus’ key goals is enhancing the understandability, or explainability, of generative AI models. This refers to the ability to pinpoint why certain outputs from LLMs are the way they are, and how customers can gain more control over those outputs’ reliability.\nPatronus incorporates features aimed at explainability, primarily by giving customers direct insight into why a particular test case passed or failed. Per Anand: “That’s something that we do with natural language explanations, and our customers have told us that they liked that, because it gives them some quick insight into what might have been the reason why things have failed—and maybe even suggestions for improvements on how they can iterate on the prompt or generation parameter values, or even for fine-tuning … Our explainability is very focused around the actual evaluation itself.”\nLooking toward the future of generative AI with AWS To build their cloud-based application, Patronus has worked with AWS since the beginning. Patronus uses a range of different cloud-based servicesL: Amazon Simple Queue Service (Amazon SQS) for queue infrastructure and Amazon Elastic Compute Cloud (Amazon EC2) for Kubernetes environments, they take advantage of the customization and flexibility available from Amazon Elastic Kubernetes Service (Amazon EKS).\nHaving worked with AWS for many years before he helped found Patronus, Anand and his team were able to leverage their familiarity and experience with AWS to quickly develop their product and infrastructure. Patronus has also worked closely with AWS’s startup-focused solutions teams, which have been “instrumental” in setting up connections and conversations. “The customer-focused aspect [at AWS] is always great, and we never take that for granted,” says Anand.\nPatronus is now looking optimistically forward, having been inundated with interest and demand in the wake of its recent launch from stealth mode with $3 million in seed funding led by Lightspeed Venture Partners. The team has also recently announced the first benchmark for LLM performance on financial questions, something co-designed with 15 financial industry domain experts.\n“We are really excited for what we’re going to be able to do in the future,” says Anand. “And we’re going to continue to be focused on AI evaluation and testing, so being able to help companies identify gaps in language models…and understand how they can quantify performance, and ultimately get better products that they can build a lot more confidence around in the future.”\nAbout the Authors Aditya Shahani Aditya Shahani is a Startup Solutions Architect focused on accelerating early stage startups throughout their journey building on AWS. He is passionate about leveraging the latest technologies to streamline business problems at scale. Bonnie McClure Bonnie is an editor specializing in creating accessible, engaging content for all audiences and platforms. She is dedicated to delivering comprehensive editorial guidance to provide a seamless user experience. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Overcoming the Challenges of Kafka Connect with Amazon Data Firehose Author: Swapna Bandla and Austin Groeneveld — July 7, 2025\nCategories: Amazon Data Firehose, Amazon Kinesis, Amazon Managed Streaming for Apache Kafka (Amazon MSK), Data Analytics, Intermediate(200)\nApache Kafka is a popular open source distributed streaming platform that is widely used in the AWS ecosystem. It’s designed to handle real-time, high-throughput data streams, making it well-suited for building real-time data pipelines to meet the streaming needs of modern cloud-based applications.\nFor AWS customers looking to run Apache Kafka, but don’t want to worry about the undifferentiated heavy lifting involved with self-managing their Kafka clusters, Amazon Managed Streaming for Apache Kafka (Amazon MSK) offers fully managed Apache Kafka. This means Amazon MSK provisions your servers, configures your Kafka clusters, replaces servers when they fail, orchestrates server patches and upgrades, makes sure clusters are architected for high availability, makes sure data is durably stored and secured, sets up monitoring and alarms, and runs scaling to support load changes. With a managed service, you can spend your time developing and running streaming event applications.\nFor applications to use data sent to Kafka, you need to write, deploy, and manage application code that consumes data from Kafka.\nKafka Connect is an open-source component of the Kafka project that provides a framework for connecting with external systems such as databases, key-value stores, search indexes, and file systems from your Kafka clusters. On AWS, our customers commonly write and manage connectors using the Kafka Connect framework to move data out of their Kafka clusters into persistent storage, like Amazon Simple Storage Service (Amazon S3), for long-term storage and historical analysis.\nAt scale, customers need to programmatically manage their Kafka Connect infrastructure for consistent deployments when updates are required, as well as the code for error handling, retries, compression, or data transformation as it is delivered from your Kafka cluster. However, this introduces a need for investment into the software development lifecycle (SDLC) of this management software. Although the SDLC is a cost-effective and time-efficient process to help development teams build high-quality software, for many customers, this process is not desirable for their data delivery use case, particularly when they could dedicate more resources towards innovating for other key business differentiators. Beyond SDLC challenges, many customers face fluctuating data streaming throughput. For instance:\nOnline gaming businesses experience throughput variations based on game usage Video streaming applications see changes in throughput depending on viewership Traditional businesses have throughput fluctuations tied to consumer activity Striking the right balance between resources and workload can be challenging. Under-provisioning can lead to consumer lag, processing delays, and potential data loss during peak loads, hampering real-time data flows and business operations. On the other hand, over-provisioning results in underutilized resources and unnecessary high costs, making the setup economically inefficient for customers. Even the action of scaling up your infrastructure introduces additional delays because resources need to be provisioned and acquired for your Kafka Connect cluster.\nEven when you can estimate aggregated throughput, predicting throughput per individual stream remains difficult. As a result, to achieve smooth operations, you might resort to over-provisioning your Kafka Connect resources (CPU) for your streams. This approach, though functional, might not be the most efficient or cost-effective solution.\nCustomers have been asking for a fully serverless solution that will not only handle managing resource allocation, but transition the cost model to only pay for the data they are delivering from the Kafka topic, instead of underlying resources that require constant monitoring and management.\nIn September 2023, we announced a new integration between Amazon MSK and Amazon Data Firehose, allowing builders to deliver data from their MSK topics to their destination sinks with a fully managed, serverless solution. With this new integration, you no longer needed to develop and manage your own code to read, transform, and write your data to your sink using Kafka Connect. Firehose abstracts away the retry logic required when reading data from your MSK cluster and delivering it to the desired sink, as well as infrastructure provisioning, because it can scale out and scale in automatically to adjust to the volume of data to transfer. There are no provisioning or maintenance operations required on your side.\nAt release, the checkpoint time to start consuming data from the MSK topic was the creation time of the Firehose stream. Firehose couldn’t start reading from other points on the data stream. This caused challenges for several different use cases.\nFor customers that are setting up a mechanism to sink data from their cluster for the first time, all data in the topic older than the timestamp of Firehose stream creation would need another way to be persisted. For example, customers using Kafka Connect connectors, like These users were limited in using Firehose because they wanted to sink all the data from the topic to their sink, but Firehose couldn’t read data from earlier than the timestamp of Firehose stream creation.\nFor other customers that were running Kafka Connect and needed to migrate from their Kafka Connect infrastructure to Firehose, this required some extra coordination. The release functionality of Firehose means you can’t point your Firehose stream to a specific point on the source topic, so a migration requires stopping data ingest to the source MSK topic and waiting for Kafka Connect to sink all the data to the destination. Then you can create the Firehose stream and restart the producers such that the Firehose stream can then consume new messages from the topic. This adds additional, and non-trivial, overhead to the migration effort when attempting to cut over from an existing Kafka Connect infrastructure to a new Firehose stream.\nTo address these challenges, we’re happy to announce a new feature in the Firehose integration with Amazon MSK. You can now specify the Firehose stream to either read from the earliest position on the Kafka topic or from a custom timestamp to begin reading from your MSK topic.\nIn the first post of this series, we focused on managed data delivery from Kafka to your data lake. In this post, we extend the solution to choose a custom timestamp for your MSK topic to be synced to Amazon S3.\nOverview of Firehose integration with Amazon MSK Firehose integrates with Amazon MSK to offer a fully managed solution that simplifies the processing and delivery of streaming data from Kafka clusters into data lakes stored on Amazon S3. With just a few clicks, you can continuously load data from your desired Kafka clusters to an S3 bucket in the same account, eliminating the need to develop or run your own connector applications. The following are some of the key benefits to this approach:\nFully managed service – Firehose is a fully managed service that handles the provisioning, scaling, and operational tasks, allowing you to focus on configuring the data delivery pipeline. Simplified configuration – With Firehose, you can set up the data delivery pipeline from Amazon MSK to your sink with just a few clicks on the AWS Management Console. Automatic scaling – Firehose automatically scales to match the throughput of your Amazon MSK data, without the need for ongoing administration. Data transformation and optimization – Firehose offers features like JSON to Parquet/ORC conversion and batch aggregation to optimize the delivered file size, simplifying data analytical processing workflows. Error handling and retries – Firehose automatically retries data delivery in case of failures, with configurable retry durations and backup options. Offset select option – With Firehose, you can select the starting position for the MSK delivery stream to be delivered within a topic from three options: Firehose stream creation time – This allows you to deliver data starting from Firehose stream creation time. When migrating from to Firehose, if you have an option to pause the producer, you can consider this option. Earliest – This allows you to deliver data starting from MSK topic creation time. You can choose this option if you’re setting a new delivery pipeline with Firehose from Amazon MSK to Amazon S3. At timestamp – This option allows you to provide a specific start date and time in the topic from where you want the Firehose stream to read data. The time is in your local time zone. You can choose this option if you prefer not to stop your producer applications while migrating from Kafka Connect to Firehose. You can refer to the Python script and steps provided later in this post to derive the timestamp for the latest events in your topic that were consumed by Kafka Connect. The following are benefits of the new timestamp selection feature with Firehose:\nYou can select the starting position of the MSK topic, not just from the point that the Firehose stream is created, but from any point from the earliest timestamp of the topic. You can replay the MSK stream delivery if required, for example in the case of testing scenarios to select from different timestamps with the option to select from a specific timestamp. When migrating from Kafka Connect to Firehose, gaps or duplicates can be managed by selecting the starting timestamp for Firehose delivery from the point where Kafka Connect delivery ended. Because the new custom timestamp feature isn’t monitoring Kafka consumer offsets per partition, the timestamp you select for your Kafka topic should be a few minutes before the timestamp at which you stopped Kafka Connect. The earlier the timestamp you select, the more duplicate records you will have downstream. The closer the timestamp to the time of Kafka Connect stopping, the higher the likelihood of data loss if certain partitions have fallen behind. Be sure to select a timestamp appropriate to your requirements. Overview of the Solution We will examine two data streaming scenarios as follows:\nWe discuss two scenarios to stream data.\nIn Scenario 1, we migrate to Firehose from Kafka Connect with the following steps: Derive the latest timestamp from MSK events that Kafka Connect delivered to Amazon S3. Create a Firehose delivery stream with Amazon MSK as the source and Amazon S3 as the destination with the topic starting position as Earliest. Query Amazon S3 to validate the data loaded. In Scenario 2, we create a new data pipeline from Amazon MSK to Amazon S3 with Firehose: Create a Firehose delivery stream with Amazon MSK as the source and Amazon S3 as the destination with the topic starting position as At timestamp. Query Amazon S3 to validate the data loaded. The solution architecture is depicted in the following diagram. Prerequisites You should have the following prerequisites:\nAn AWS account and access to the following AWS services: Amazon Elastic Compute Cloud (Amazon EC2) Amazon Data Firehose AWS Identity and Access Management (IAM) Amazon MSK Amazon S3 An MSK provisioned or MSK serverless cluster with topics created and data streaming to it. The sample topic used in this is order. An EC2 instance configured to use as a Kafka admin client. Refer to Create an IAM role for instructions to create the client machine and IAM role that you will need to run commands against your MSK cluster. An S3 bucket for delivering data from Amazon MSK using Firehose. Kafka Connect to deliver data from Amazon MSK to Amazon S3 if you want to migrate from Kafka Connect (Scenario 1). Migrate to Firehose from Kafka Connect To reduce duplicates and minimize data loss, you need to configure your custom timestamp for Firehose to read events as close to the timestamp of the oldest committed offset that Kafka Connect reported. You can follow the steps in this section to visualize how the timestamps of each committed offset will vary by partition across the topic you want to read from. This is for demonstration purposes and doesn’t scale as a solution for workloads with a large number of partitions.\nSample data was generated for demonstration purposes by following the instructions referenced in the following GitHub repo. We set up a sample producer application that generates clickstream events to simulate users browsing and performing actions on an imaginary ecommerce website.\nTo derive the latest timestamp from MSK events that Kafka Connect delivered to Amazon S3, complete the following steps:\nFrom your Kafka client, query Amazon MSK to retrieve the Kafka Connect consumer group ID: ./kafka-consumer-groups.sh --bootstrap-server $bs --list --command-config client.properties Stop Kafka Connect. Query Amazon MSK for the latest offset and associated timestamp for the consumer group belonging to Kafka Connect. You can use the get_latest_offsets.py Python script from the following GitHub repo as a reference to get the timestamp associated with the latest offsets for your Kafka Connect consumer group. To enable authentication and authorization for a non-Java client with an IAM authenticated MSK cluster, refer to the following GitHub repo for instructions on installing the aws-msk-iam-sasl-signer-python package for your client.\npython3 get_latest_offsets.py --broker-list $bs --topic-name “order” --consumer-group-id “connect-msk-serverless-connector-090224” --aws-region “eu-west-1” Note the earliest timestamp across all the partitions.\nCreate a data pipeline from Amazon MSK to Amazon S3 with Firehose The steps in this section are applicable to both scenarios. Complete the following steps to create your data pipeline:\nOn the Firehose console, choose Firehose streams in the navigation pane. Choose Create Firehose stream. For Source, choose Amazon MSK. For Destination, choose Amazon S3. For Source settings, browse to the MSK cluster and enter the topic name you created as part of the prerequisites. Configure the Firehose stream starting position based on your scenario: For Scenario 1, set Topic starting position as At Timestamp and enter the timestamp you noted in the previous section. For Scenario 2, set Topic starting position as Earliest. For Firehose stream name, leave the default generated name or enter a name of your preference. For Destination settings, browse to the S3 bucket created as part of the prerequisites to stream data. Within this S3 bucket, by default, a folder structure with YYYY/MM/dd/HH will be automatically created. Data will be delivered to subfolders pertaining to the HH subfolder according to the Firehose to Amazon S3 ingestion timestamp.\nUnder Advanced settings, you can choose to create the default IAM role for all the permissions that Firehose needs or choose existing an IAM role that has the policies that Firehose needs. Choose Create Firehose stream. On the Amazon S3 console, you can verify the data streamed to the S3 folder according to your chosen offset settings.\nClean up To avoid incurring future charges, delete the resources you created as part of this exercise if you’re not planning to use them further.\nConclusion Firehose provides a straightforward way to deliver data from Amazon MSK to Amazon S3, enabling you to save costs and reduce latency to seconds. To try Firehose with Amazon S3, refer to the Delivery to Amazon S3 using Amazon Data Firehose lab.\nAbout the Authors Aditya Shahani Aditya Shahani is a Startup Solutions Architect focused on accelerating early stage startups throughout their journey building on AWS. He is passionate about leveraging the latest technologies to streamline business problems at scale. Bonnie McClure Bonnie is an editor specializing in creating accessible, engaging content for all audiences and platforms. She is dedicated to delivering comprehensive editorial guidance to provide a seamless user experience. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"How Brisa Robotics Uses AWS to Improve Robotics Operations by Erica Goldberger and Sophie Pagalday on 24 FEB 2023\nin Amazon Kinesis, Amazon Simple Storage Service (S3), Amazon Timestream, Analytics, AWS IoT Greengrass, AWS Lambda, Customer Solutions, Database, Internet of Things, Kinesis Data Analytics, Robotics, Storage, Technical How-to | Permalink | Share\nIn this post, you’ll learn how Brisa Robotics leverages Amazon Web Services (AWS) to collect, store, and process data from mixed fleets of vehicles to improve customer operations.\nBrisa transforms non-autonomous machines into fleets of autonomous vehicles that collect data to help customers track key performance metrics and improve operations. Their mission is to enhance its customers’ efficiencies by leveraging existing infrastructure and reusing old machines instead of selling scraps and buying new ones. Brisa provides unique modular robotic kits to enhance material handling equipment (MHE) such as forklifts, palletizers, and telehandlers. These robotic kits are retrofitted for the MHE to include Brisa’s proprietary data collection platform. The kits support different use cases: stock-keeping unit (SKU) tracking, inspection (defects, objects, barcodes), and material movement. Getting better visibility into these use cases with data and metrics allows Brisa’s customers to optimize their warehouse layout and plan better.\nChallenge: Build a Flexible Data Collection Solution The largest brewing company in the world needed more visibility into their warehouse operations and stock to increase productivity and improve safety. So Brisa set out to create a solution to collect and stream data their customer could use to make better decisions.\nBrisa is committed to avoiding infrastructure changes for customers so as not to increase customer overhead and maintenance costs. They wanted to help their customer without requiring them to change any of their existing infrastructures. Furthermore, Brisa needed to provide a single solution that would work for their customers with different workflows and requirements.\nDepending on the customer, Brisa has different requirements to consider. For example, some customers want their data dashboard only available in their network, whereas others want it online. Additionally, some customers want a local computer that collects the data before sending it to the cloud, whereas others want the data transmitted directly from the robots. Brisa needed a flexible tool to run on different platforms for different scenarios.\nThe goal was to develop a flexible solution to collect and expose data and metrics for varied customers without customers needing to change any infrastructure.\nSolution Overview Brisa developed a solution that collects data from the MHE robots; streams, processes, and stores it in AWS; and then pulls it into live custom dashboards for customers. This outcome occurs without changing the customer infrastructure, and the workflow can function with or without a stable internet connection.\nAWS IoT Greengrass V2 components are deployed to the robots, including pre-built components such as the stream manager. Data is collected from the client application running on the server (either the robot or an external machine on the robot network). This application can be run as a custom Greengrass component or outside of Greengrass. The stream manager component streams data directly to Amazon Kinesis Data Streams (Amazon KDS) and Amazon Simple Storage Service (Amazon S3). A Python based AWS Lambda function processes the raw data from the Kinesis data stream and stores it in an Amazon Timestream database. Once data is in AWS, Brisa’s web application can query Amazon Timestream to collect data for their dashboards. You will learn more about this workflow below.\nCollecting the Data Brisa collects data on the robots, such as object detection, robot position, robot speed, fork movements, and system monitoring. They do this using Robot Operating System 2 (ROS2), an open-source set of libraries and tools for building robot applications. ROS2 enables Brisa to construct and develop robot applications faster by using community-built nodes and devices such as simulation and build tooling. As a founding technical steering committee member for ROS2, AWS is a vital participant in the community, which results in a wide array of options for running ROS2 tooling in AWS. AWS gives Brisa the most scalable cloud platform with the deepest integrations with ROS2.\nStreaming the Data Brisa subscribes to the ROS2 topic and forwards those events into the Kinesis data stream using the AWS IoT Greengrass V2 stream manager. AWS IoT Greengrass is an open-source Internet of Things (IoT) edge runtime and cloud service that helps you build, deploy and manage IoT applications on your devices. You can use AWS IoT Greengrass to build edge applications using pre-built or custom software modules, called components, that can connect your edge devices to AWS services or third-party services. The stream manager component enables you to process data streams to transfer to the AWS Cloud from Greengrass core devices.\nBrisa chose this Greengrass stream manager because it can run offline, under intermittent network conditions, without you having to worry about buffering and publishing data into AWS. The data is stored locally and compressed until an internet connection is active. Instead of managing this workflow, Brisa can send the data to the stream and focus on its unique robotic workflows. This setup is flexible to different customer needs as the Greengrass stream manager runs on the robots themselves or the local computer where the robots send data.\nBrisa has a client application running and a server to start the stream manager. Depending on the customer, this server can be on the robot or an external machine on the robot network. For more information on setting up the stream manager, see the stream manager documentation and Deploy and Manage ROS Robots with AWS IoT Greengrass V2.\nBrisa then used a ROS2 node to collect data from the sensors. They did this by creating a ROS2 package.\nSample commands to create a new ROS2 package:.\nSample commands to create a new ROS2 package: cd ~ mkdir -p ws/src pip install stream_manager cd src ros2 pkg create \\ --package-format 3 \\ --build-type ament_python \\ sm_upload Brisa publishes data to the Kinesis data stream and an Amazon S3 bucket through the Greengrass stream manager. They accomplish this by leveraging the stream manager Python SDK in their ROS nodes. Below is a sample ROS node similar to Brisa’s implementation that publishes data from ROS to the stream manager:\nimport json import rclpy from rclpy.node import Node from stream_manager import ( ExportDefinition, KinesisConfig, MessageStreamDefinition, StrategyOnFull, StreamManagerClient, ) STREAM_NAME = \u0026#34;SomeStream\u0026#34; KINESIS_STREAM_NAME = \u0026#34;MyKinesisStream\u0026#34; class StreamManagerPublisher(Node): def __init__(self): super().__init__(\u0026#34;aws_iot_core_publisher\u0026#34;) timer_period = 3 # seconds self.client = StreamManagerClient() exports = ExportDefinition( kinesis=[ KinesisConfig( identifier=\u0026#34;KinesisExport\u0026#34; + STREAM_NAME, kinesis_stream_name=KINESIS_STREAM_NAME, ) ] ) # Create the Status Stream if it does not exist already try: self.client.create_message_stream( MessageStreamDefinition( name=STREAM_NAME, strategy_on_full=StrategyOnFull.OverwriteOldestData, export_definition=exports, ) ) except ConnectionRefusedError as e: self.get_logger().error(f\u0026#34;Could not connect to the stream manager: {str(e)}\u0026#34;) raise except Exception: pass # Create the message stream with the S3 Export definition. self.client.create_message_stream( MessageStreamDefinition( name=STREAM_NAME, strategy_on_full=StrategyOnFull.OverwriteOldestData, export_definition=exports, ) ) self.timer = self.create_timer(timer_period, self.timer_callback) def timer_callback(self): self.client.append_message(STREAM_NAME, json.dumps({\u0026#34;robot_id\u0026#34;: \u0026#34;C3PO\u0026#34;,\u0026#34;timestamp\u0026#34;: datetime.datetime.utcnow().isoformat(),\u0026#34;x\u0026#34;: 1.0, \u0026#34;y\u0026#34;: 1.1, \u0026#34;z\u0026#34;: 3.0}).encode(\u0026#34;utf-8\u0026#34;)) self.get_logger().info(\u0026#34;Successfully appended S3 Task Definition to stream\u0026#34;) def main(args=None): rclpy.init(args=args) sm_publisher = StreamManagerPublisher() rclpy.spin(sm_publisher) sm_publisher.destroy_node() rclpy.shutdown() if __name__ == \u0026#34;__main__\u0026#34;: main() Then they built the ROS2 package:\ncolcon build --packages-up-to sm_upload source install/setup.bash ros2 run sm_upload sm_upload Storing the Data Brisa uses the stream manager to stream some of the data, such as images and video, to Amazon S3 for object storage. The rest of the data, such as telemetry data, is streamed to Amazon Kinesis Data Streams, processed with an AWS Lambda function, and then stored in Amazon Timestream, a purpose-built time-series database.\nAmazon Kinesis Data Streams helps ingest and collect data from application and service logs and deliver data into data lakes. See the Create a data stream to learn more about creating a stream.\nBrisa uses an AWS Lambda function to orchestrate the extract, process, and load (ETL) operations from Amazon Kinesis into Amazon Timestream. They chose Lambda because it is serverless, so the cost is based on the number of requests and their duration (the time it takes for your code to run). They explored other options with managed ETL features in AWS but found using a simple Lambda function was the most suitable approach for their current ETL requirements.\nQuerying Newly Stored Data for Their Dashboards Once the data is in Timestream, Brisa can leverage time series functions to make simple yet efficient queries to show the custom time based metrics. From the Amazon Timestream console, Brisa can test SQL-like requests. For a more detailed explanation of these concepts, see Timestream Concepts and Using the console.\nHere is an example query Brisa can run to extract interesting business data. In this query, positions are grouped by 5 second frames to get an average of x and y coordinates. This is useful to optimize query costs by not fetching all data points, all the time.\nSELECT ROUND(AVG(x), 2) AS avg_x, ROUND(AVG(y), 2) AS avg_y, BIN(time, 5s) AS binned_timestamp FROM database.table WHERE x IS NOT NULL AND y IS NOT NULL AND robot_name=\u0026#34;C3PO\u0026#34; GROUP BY BIN(time, 5s) ORDER BY binned_timestamp ASC Brisa is also able to query the collected data from different SDKs such as boto3 for Python or AWSJavascriptSDK for JavaScript. A full list of available programming languages can be found in Tools to Build on AWS.\nBrisa then uses these queries to pull the relevant data into their customer dashboards.\nBrisa makes Timestream queries from their backend using AWS SDK for panda (previously AWS Data Wrangler) to get and arrange data for the dashboard and API. The frontend then displays this data on the dashboard.\nResults Once the data is in AWS, Brisa can easily query the collected information, adapt it, and expose it as a live dashboard and KPI reports for customers. Brisa’s library of metrics and visualizations is modular and dynamic and is continuously updated to account for new integrations and use cases depending on customer needs. This capability allows customers to improve overall safety conditions and efficiency in their warehouse.\nBefore Brisa’s solution, the brewing company was primarily handling their tracking manually. Thanks to the dashboard and report, Brisa can now provide them with:\nHigher accuracy: The stored bottles are not easily identifiable by a person since they are 5 meters high. This makes tracking difficult. Brisa’s solution collects camera images for their customer, allowing stored bottles to be identified accurately. Increased data frequency: The autonomous robot is able to scan twice as often than any person manually can. Improved safety: Forklifts are often on the same alleys as a human inventory counter. Having a robot handle counting, improves safety by removing the risk of accidents between dangerous forklifts and human counters. Better operational insights: Heatmaps in the live dashboards allow Brisa’s customers to identify bottlenecks in their operations and improve scheduling. This type of operational insight would not be possible by human observation. Here are some screenshots taken from Brisa’s dashboard:\nPosition heatmap in Brisa’s in-house testing area. Ideally, there shouldn’t be any area where the robot stopped, but looking at this heatmap, you can observe some areas where the robot stopped more often than others. There can be multiple reasons for this: non-optimal robot routes, lousy signage, bad planning, or other more severe problems, such as a person or other robot blocking the area. To solve this, Brisa also collects camera images so their customer can look at what happened. The images allow the brewing company to understand how to improve the layout or plan differently, so the robot moves steadily without pausing its operations.\nDashboard with the camera image and robot view. The dashboard grabs these files from Amazon S3 where the data was stored by the stream manager.\nDashboard showing images from the customer site.\nDashboard showing some metrics Brisa shows customers. These metrics are all queried from Timestream. Apart from standard metrics (such as duration and distance), the modular design using AWS enabled Brisa to easily provide custom integrations for their customer such as:\nNumber of vehicle stops: Vehicle stops is a metric used internally at the brewing company. The dashboard indicates how long each stop lasts. The customer can click on the dashboard results to see related events that happened at this time. For example, for a vehicle stop, they may see a picture where a person was detected. The user can then click on the image to see where in the warehouse this occurred and at what time. This helps the customer understand stopping patterns over time to improve efficiency in their warehouse. Load summary: The robot should move most of the time except when charging and for limited stops. Brisa customizes the dashboard depending on the fleet, the robots running, and the KPIs that matter to each customer. With this data streaming pipeline, Brisa could provide their customer with tools to track their stock and get deeper insights into operational issues without modifying their workflow. This capability helps customers such as the brewing company improve their warehouse operational efficiency and safety.\nBrisa is integrating with more clients and types of robots, adding insightful and customized metrics to its dashboard constantly.\nBrisa can help you control your stock more often and precisely, identify and solve bottlenecks in your operations (manual and/or autonomous), and make decisions based on actual data. To learn more, check out the website at www.brisa.tech.\nTAGS: autonomous robots, AWS Robotics, Cloud Robotics\nAbout the Authors Aditya Shahani Aditya Shahani is a Startup Solutions Architect focused on accelerating early stage startups throughout their journey building on AWS. He is passionate about leveraging the latest technologies to streamline business problems at scale. Bonnie McClure Bonnie is an editor specializing in creating accessible, engaging content for all audiences and platforms. She is dedicated to delivering comprehensive editorial guidance to provide a seamless user experience. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"TaskHub – A DevSecOps-Based Task and Progress Management Platform on AWS 1. Executive Summary TaskHub is a task and progress management platform designed to help small and medium-sized businesses as well as working teams manage workloads, deadlines, and progress in a visual and secure manner.\nThe system is developed following the DevSecOps model and is fully built on AWS Serverless, ensuring scalability, security, and cost optimization.\nThe development and deployment process adopts AWS CodePipeline and CodeBuild to automate CI/CD and security testing.\n2. Problem Statement Problems:\nSmall businesses and project teams often face difficulties in managing workload, tracking progress, and distributing tasks among members.\nPopular tools such as Jira or Asana usually come with high costs and lack seamless integration with DevSecOps processes or AWS environments.\nSolutions:\nTaskHub leverages a Serverless AWS architecture to build a lightweight, secure, and cost-efficient platform.\nThe platform is developed using AWS Lambda, API Gateway, DynamoDB, Cognito, and S3/CloudFront, while AWS CodePipeline is integrated for CI/CD and dynamic security testing.\nBenefits and Return on Investment (ROI)\nTaskHub delivers practical benefits for development teams and small-to-medium enterprises. The system functions as a centralized platform for task management, progress tracking, and role-based member management. The adoption of AWS Serverless architecture significantly reduces operational costs, optimizes resource utilization, and enhances scalability as usage demand grows. Furthermore, the platform serves as a hands-on DevSecOps practice environment, enabling research and development teams to expand future projects. According to estimates from the AWS Pricing Calculator, the system operation cost is approximately 0.66 USD per month, equivalent to 7.92 USD per year. Since the entire infrastructure is based on shared AWS services, no physical hardware investment is required. The expected break-even time is within 6–12 months, thanks to substantial reductions in manual management effort and optimized internal workflows.\n3. Solution Architecture TaskHub is built upon an AWS Serverless architecture, ensuring operational scalability, high performance, and cost efficiency. The platform focuses on real-time task, team, and project progress management while maintaining an automated DevSecOps deployment pipeline.\nThe overall architecture includes key components such as Amazon API Gateway, which acts as the request entry point and traffic distributor, AWS Lambda for backend business logic processing, and Amazon DynamoDB for storing task data, user information, and access control.\nThe web frontend is hosted on Amazon S3 and globally distributed via Amazon CloudFront, while AWS Cognito handles authentication and authorization.\nThe CI/CD process is fully automated using AWS CodePipeline combined with AWS CodeBuild, enabling continuous deployment and security validation without server management.\nThe entire architecture is protected by AWS WAF and AWS KMS to enhance security and enforce DevSecOps compliance. AWS X-Ray is used for performance tracing and latency analysis. The complete architecture is illustrated in the diagram below:.\nAWS Services Utilized Amazon Route 53: Highly reliable DNS service for traffic routing.\nAWS WAF (Web Application Firewall): Advanced protection layer against common web attacks.\nAmazon CloudFront: Global content delivery for the user interface and static assets.\nAmazon S3 (Simple Storage Service): Static hosting of all frontend build files (Next.js).\nAmazon Cognito: User authentication and authorization management.\nAmazon API Gateway: Middleware layer for authentication and API routing to Lambda.\nAWS Lambda: Core business logic processing and CloudWatch logging integration.\nAmazon DynamoDB: High-performance NoSQL database encrypted using AWS KMS.\nAWS SNS (Simple Notification Service): Asynchronous notification service.\nAWS Secrets Manager: Secure storage, management, and rotation of sensitive credentials.\nAWS CodePipeline, CodeBuild \u0026amp; CodeGuru\nCodePipeline/CodeBuild: CI/CD automation; CodeBuild performs static security testing (SAST).\nAWS CodeGuru: Automated source code analysis tool integrated into CI/CD to provide intelligent recommendations for performance optimization and code quality improvement, especially for Lambda environments.\nAWS CloudFormation: Infrastructure as Code (IaC) service for automated resource provisioning.\nAWS CloudWatch Logs \u0026amp; AWS X-Ray: CloudWatch Logs collects logs, CloudWatch sets alarms, and X-Ray provides distributed tracing and performance insights.\nComponent Design 1. Frontend Layer: User Interface: Built with Next.js as a Static Site.\nStorage \u0026amp; Distribution: Static files are securely hosted on Amazon S3 and globally delivered via Amazon CloudFront, protected by AWS WAF at the Edge layer.\n2. Backend Layer: API Gateway: Amazon API Gateway receives all requests and verifies authentication using Cognito Authorizer.\nProcessing: AWS Lambda Functions handle business logic (task CRUD, team management, role authorization).\nSecrets Management: Each Lambda function securely accesses sensitive credentials via AWS Secrets Manager, preventing secrets from being hardcoded.\n3. Data Layer: Database: Amazon DynamoDB stores task data, progress, and user configurations in On-Demand mode for auto-scaling and cost efficiency.\nData Security: All at-rest data in DynamoDB is encrypted using AWS KMS (Key Management Service).\n4. Security \u0026amp; Authentication: Authentication: Amazon Cognito manages login, sessions, and role-based access control (RBAC), supporting Multi-Factor Authentication (MFA) and Single Sign-On (SSO).\nEdge Protection: AWS WAF is placed in front of CloudFront to mitigate Layer 7 DDoS attacks and OWASP Top 10 threats.\n5. Deployment \u0026amp; Monitoring: DevSecOps CI/CD: Source code is hosted on GitLab and automated by AWS CodePipeline/CodeBuild, including CodeGuru analysis before infrastructure deployment via CloudFormation.\nMonitoring \u0026amp; Debugging: CloudWatch Logs collects service logs; CloudWatch creates automated alarms; AWS X-Ray provides tracing and latency optimization.\n4. Technical Implementation The TaskHub project is divided into two main areas—AWS Serverless infrastructure development and task management platform development—each consisting of the following phases:\nDevelopment Phases Phase 1: Design \u0026amp; Modeling (Month 1) Key Actions: Research on Serverless/DevSecOps, selection of core services (Lambda, DynamoDB, API Gateway), detailed architecture design, and NoSQL data modeling.\nDeliverables: Architecture Diagram and Data Model Documentation.\nPhase 2: Infrastructure as Code Initialization (Month 2) Key Actions: Detailed cost estimation; AWS CDK development for foundational services (S3, CloudFront, Cognito).\nDeliverables: Base AWS CDK source code and Cost Estimation Report.\nPhase 3: DevSecOps Automation Setup (Months 2–3) Key Actions: Full CI/CD pipeline setup (CodePipeline/CodeBuild), integration of AWS CodeGuru and SAST for automated code quality and security scanning.\nDeliverables: Operational CodePipeline and automated security scanning workflow.\nPhase 4: Development \u0026amp; Deployment (Months 3–4) Key Actions: Feature development (Lambda with TypeScript, frontend with Next.js), integration testing, and production deployment via Pipeline.\nDeliverables: TaskHub Beta Version (Full CRUD) and Testing Report.\nTechnical Requirements Architecture \u0026amp; Tools: Entire system is managed using AWS CDK for infrastructure consistency.\nTechnology Stack: Backend uses TypeScript/Node.js; frontend uses Next.js (React).\nSource Code Management: GitLab with automated deployment via AWS CodePipeline.\nMonitoring: CloudWatch, X-Ray, and CloudWatch Logs for deep performance monitoring and debugging.\nNon-functional Requirements: Deployed in Singapore region (ap-southeast-1) for optimal Vietnam latency, scalable to 50 users, and encrypted using AWS KMS.\n5. Timeline \u0026amp; Key Milestones Project Timeline Pre-Internship (Month 0): Planning, DevSecOps research, and AWS Serverless service study.\nMonth 1: Development environment setup, AWS infrastructure initiation, and CI/CD pipeline initialization.\nMonth 2: Architecture design, core feature implementation, and automated security testing.\nMonth 3: Frontend-backend integration, beta development, and platform release.\nPost-Release: Maintenance, performance evaluation, and feature expansion.\n6. Budget Estimation Hourly Rates Resource Responsibility Rate (USD)/Hour Solution Architect [1] System \u0026amp; API design, Database design, Technical leadership 6 Engineers [3] Backend, Frontend, Security implementation 4 Others (DevOps) [1] CI/CD, Cloud deployment, Monitoring, Security 4 Project Phases Cost Breakdown Project Phase Architect Engineers Others Total Hours System \u0026amp; Architecture Design 20 10 0 30 Backend Development 10 80 0 90 Frontend Development 5 60 0 65 Security \u0026amp; CI/CD Setup 5 30 10 45 Testing \u0026amp; Deployment 5 30 0 35 Total Hours 45 210 10 265 Total Cost (USD) 270 840 40 800 Cost Contribution Distribution Party Contribution (USD) Percentage Customer 0 0% Partner 0 0% AWS 800 100% 7. Risk Assessment Risk Matrix AWS service disruption: Medium impact – Medium probability CI/CD misconfiguration: High impact – Low probability AWS budget overrun: Medium impact – Low probability Security vulnerabilities: High impact – Medium probability Performance degradation under load: Medium impact – Medium probability Mitigation Strategies Multi-region monitoring using CloudWatch and X-Ray\nCode review and validation before deployment via CodePipeline\nCost monitoring using AWS Budgets\nAutomated security scanning via CodeBuild (replacing GitHub Actions)\nContingency Plan Maintain staging environments for rapid recovery\nUse CloudFormation and AWS Backup for configuration and data backup\n8. Expected Outcomes Technical Improvements Full Automation: A complete transition to automated DevSecOps, achieving deployment times under 6 minutes.\nGuaranteed Performance: API response time under 150ms and system availability at 99.9% uptime.\nIntegrated Security: Automated detection and remediation of critical security issues during the build phase.\nScalability Ready: The platform is capable of scaling to support high traffic and user growth without architectural changes.\nLong-Term Value Reusable Technical Assets: A complete set of AWS CDK/CloudFormation templates optimized for cost and scalability, reusable for future projects.\nSolid Platform Foundation: An industry-standard DevSecOps-based task management environment ready for long-term feature expansion.\n"},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"AWS Cloud Mastery Series #2 – DevOps on AWS 1. Event Information Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS Time: 8:30 – 17:00 Organizer: Amazon Web Services (AWS) 2. Purpose of Participation I attended this workshop to understand DevOps culture and principles in cloud environments, learn how to build a complete CI/CD pipeline, master Infrastructure as Code (IaC) with CloudFormation and CDK, explore container services on AWS, and implement monitoring and observability using CloudWatch and X-Ray.\nI also wanted to apply DevOps best practices through real case studies.\n3. Agenda Summary 8:30 – 9:00 — Welcome \u0026amp; DevOps Mindset Main activities:\nCheck-in and review of previous AI/ML session Introduction to DevOps culture and principles Key metrics (DORA, MTTR, deployment frequency) Importance of DevOps in digital transformation Key insight:\nUnderstood that DevOps is not just a toolset but a culture, learned essential performance metrics, and how DevOps improves real-world delivery efficiency.\n9:00 – 10:30 — AWS DevOps Services – CI/CD Pipeline Main content:\nSource Control with AWS CodeCommit + Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test automation using CodeBuild Deployment with CodeDeploy: Blue/Green, Canary, Rolling updates Orchestration using CodePipeline Live Demo: Complete CI/CD pipeline walkthrough Key insight:\nDeep understanding of CI/CD from source to deployment, differences between deployment strategies, and hands-on experience with AWS DevOps services.\n10:30 – 10:45 — Break Networking and discussions with AWS experts and other participants.\n10:45 – 12:00 — Infrastructure as Code (IaC) Main content:\nAWS CloudFormation: templates, stacks, drift detection AWS CDK: constructs, reusable patterns, language support Demo: Deploying with CloudFormation \u0026amp; CDK Discussion: Choosing the right IaC tool Key insight:\nGained in-depth understanding of IaC, comparison of CloudFormation and CDK, and techniques to manage infrastructure efficiently and reusable.\n13:00 – 14:30 — Container Services on AWS Main content:\nDocker fundamentals: microservices \u0026amp; containerization Amazon ECR: image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: scaling, deployment, orchestration AWS App Runner: simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison Key insight:\nMastered containerization concepts, understood ECS vs EKS, learned how to manage images via ECR, and gained hands-on microservices deployment experience.\n14:45 – 16:00 — Monitoring \u0026amp; Observability Main content:\nCloudWatch: metrics, logs, alarms, dashboards AWS X-Ray: distributed tracing \u0026amp; performance insights Demo: Building complete observability Best Practices: alerting, dashboards, on-call process Key insight:\nLearned how to build effective monitoring systems, use CloudWatch + X-Ray for full application visibility, and apply modern observability practices.\n16:00 – 16:45 — DevOps Best Practices \u0026amp; Case Studies Main content:\nDeployment strategies: feature flags, A/B testing Automated testing \u0026amp; CI/CD integration Incident management \u0026amp; postmortems Case studies: DevOps transformation in startups \u0026amp; enterprises Key insight:\nGained practical insights into advanced deployment techniques, automated testing integration, and professional incident-handling workflows.\n16:45 – 17:00 — Q\u0026amp;A \u0026amp; Wrap-up Main content:\nDevOps career paths and job opportunities AWS certification roadmap Open discussion and final Q\u0026amp;A Key Takeaways Comprehensive understanding of DevOps culture on AWS Mastery of CI/CD pipeline using CodeCommit, CodeBuild, CodeDeploy, and CodePipeline Strong knowledge of IaC with CloudFormation and CDK Deep understanding of AWS container services (ECR, ECS, EKS, App Runner) Ability to set up monitoring and observability with CloudWatch \u0026amp; X-Ray Applied DevOps best practices: deployment strategies, automated testing, incident management Real-world knowledge from case case studies and demos Practical Application Build CI/CD pipelines for current and future projects Use Infrastructure as Code to manage environments Implement microservices containerization Set up effective monitoring \u0026amp; alerting systems Apply deployment strategies like Blue/Green \u0026amp; Canary Improve incident response and postmortem processes Event Experience Attending “DevOps on AWS” was extremely valuable, giving me a deeper understanding of how to apply DevOps practices in real AWS cloud environments.\nHighlights AWS Experts: Gained insights from experienced AWS Solutions Architects, learned DevOps mindset with real examples Hands-on Demos: Observed full CI/CD pipeline, IaC deployment with CloudFormation \u0026amp; CDK Real Case Studies: Learned DevOps transformations from both startups and enterprises High-quality Networking: Connected with DevOps Engineers \u0026amp; Cloud Architects, exchanged practical challenges and solutions Important Lessons DevOps is a journey, not a destination — continuous improvement is essential IaC is the foundation for scalable, reliable DevOps Monitoring \u0026amp; observability are key to system reliability Container technology is the future of modern application deployment Event Photos "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Understand the basic about AWS Virtual Private Cloud Learn about VPC Security and Multi-VPC Features Understand the basic of the VPC, DirectConnect, LoadBalancer, ExtraResources Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study the knowledge related to Virtual Private Cloud - Firewall in VPC - Practice: + Create VPC, Subnet, Internet Gateway\n+ Create Route Table, Security Group + Enable VPC Flow Logs - Deploy Amazon EC2 Instances 09/15/2025 09/15/2025 https://000003.awsstudygroup.com/1-introduce/ 3 - Set up Hybrid DNS with Route 53 Resolver - Practice: + Generate Key Pair + Intialize CloudFormation Template + Configuring Security Group + Connecting to RDGW 09/16/2025 09/16/2025 https://000010.awsstudygroup.com/3-connecttordgw/ 4 - Learn about VPC Peerings and How to set up - Practice: + Initialize CloudFormation Template + Create Security Group + Create EC2 Instance + Update Network ACL + Configuring route tables to enable communication between peered VPCs + Enabling and testing Cross-Peer DNS 09/17/2025 09/17/2025 https://000019.awsstudygroup.com/6-crosspeerdns/ 5 - Learn basic AWS Transit Gateway - Practice: + Create Transit Gateway + Create Transit Gateway Attachments + Create Transit Gateway Route Tables + Add Transit Gateway Routes to VPC Route Tables 08/18/2025 08/18/2025 https://000020.awsstudygroup.com/2-prerequiste/ 6 - Synthesize knowledge on Networking fundamentals (VPC, Subnets, Gateways, Routing) - Do some exercises on VPC Peering and connecting VPCs using the Transit Gateway 08/19/2025 08/19/2025 Week 2 Achievements: VPC Foundation Mastery: Successfully created and configured core Virtual Private Cloud (VPC) components: Subnets, Internet Gateway, Route Tables, and Security Groups. Core Deployment \u0026amp; Monitoring: Gained practical experience deploying Amazon EC2 Instances within the custom VPC environment. Networking Security \u0026amp; Monitoring: Applied security principles by configuring Firewall mechanisms (Security Groups and Network ACLs) and successfully enabled VPC Flow Logs for traffic inspection. Multi-VPC Connectivity: Mastered the setup and communication for complex networking scenarios: VPC Peering: Configured route tables and tested Cross-Peer DNS between peered VPCs. Transit Gateway (TGW): Learned and implemented TGW, managing attachments and route tables for scalable VPC interconnectivity. Hybrid DNS \u0026amp; Access: Practiced setting up Hybrid DNS using Route 53 Resolver and configured secure connection methods (Key Pair generation, RDGW access) via CloudFormation templates. Knowledge Synthesis: Consolidated all weekly networking knowledge by completing integrated exercises on complex multi-VPC routing and security. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.8-cognito/5.8.2-password-policies/","title":"Configure Password Policies","tags":[],"description":"","content":"Configure Password Policies In this step, you will configure password policies to ensure strong password requirements for your users.\nNavigate to your User Pool in the Cognito console Go to Authentication methods tab Click Edit in the Password policy section Password Policy Configuration Configure the following password requirements:\nPassword length:\nMinimum length: 8 characters Maximum length: 256 characters Password complexity:\n✅ Require numbers ✅ Require special characters ✅ Require uppercase letters ✅ Require lowercase letters Save Configuration Review your password policy settings Click Save changes Verify Password Policy The password policy is now active. Users will need to create passwords that meet these requirements:\nExample of valid passwords:\nMySecurePass123! StrongPassword2024# TaskManager@2025 Example of invalid passwords:\npassword (no uppercase, numbers, special chars) 12345678 (no letters, special chars) Pass1! (too short) "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3FullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:DeleteBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketCORS\u0026#34;, \u0026#34;s3:PutBucketCORS\u0026#34;, \u0026#34;s3:GetBucketWebsite\u0026#34;, \u0026#34;s3:PutBucketWebsite\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:PutBucketVersioning\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFrontFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateDistribution\u0026#34;, \u0026#34;cloudfront:GetDistribution\u0026#34;, \u0026#34;cloudfront:GetDistributionConfig\u0026#34;, \u0026#34;cloudfront:UpdateDistribution\u0026#34;, \u0026#34;cloudfront:DeleteDistribution\u0026#34;, \u0026#34;cloudfront:ListDistributions\u0026#34;, \u0026#34;cloudfront:CreateInvalidation\u0026#34;, \u0026#34;cloudfront:GetInvalidation\u0026#34;, \u0026#34;cloudfront:ListInvalidations\u0026#34;, \u0026#34;cloudfront:CreateOriginAccessControl\u0026#34;, \u0026#34;cloudfront:GetOriginAccessControl\u0026#34;, \u0026#34;cloudfront:UpdateOriginAccessControl\u0026#34;, \u0026#34;cloudfront:DeleteOriginAccessControl\u0026#34;, \u0026#34;cloudfront:ListOriginAccessControls\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;WAFAndShieldAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;wafv2:CreateWebACL\u0026#34;, \u0026#34;wafv2:GetWebACL\u0026#34;, \u0026#34;wafv2:UpdateWebACL\u0026#34;, \u0026#34;wafv2:DeleteWebACL\u0026#34;, \u0026#34;wafv2:ListWebACLs\u0026#34;, \u0026#34;wafv2:AssociateWebACL\u0026#34;, \u0026#34;wafv2:DisassociateWebACL\u0026#34;, \u0026#34;wafv2:CreateIPSet\u0026#34;, \u0026#34;wafv2:GetIPSet\u0026#34;, \u0026#34;wafv2:UpdateIPSet\u0026#34;, \u0026#34;wafv2:DeleteIPSet\u0026#34;, \u0026#34;wafv2:ListIPSets\u0026#34;, \u0026#34;wafv2:CreateRuleGroup\u0026#34;, \u0026#34;wafv2:GetRuleGroup\u0026#34;, \u0026#34;wafv2:UpdateRuleGroup\u0026#34;, \u0026#34;wafv2:DeleteRuleGroup\u0026#34;, \u0026#34;wafv2:ListRuleGroups\u0026#34;, \u0026#34;shield:DescribeSubscription\u0026#34;, \u0026#34;shield:GetSubscriptionState\u0026#34;, \u0026#34;shield:DescribeProtection\u0026#34;, \u0026#34;shield:ListProtections\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CognitoFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cognito-idp:CreateUserPool\u0026#34;, \u0026#34;cognito-idp:DeleteUserPool\u0026#34;, \u0026#34;cognito-idp:DescribeUserPool\u0026#34;, \u0026#34;cognito-idp:ListUserPools\u0026#34;, \u0026#34;cognito-idp:UpdateUserPool\u0026#34;, \u0026#34;cognito-idp:CreateUserPoolClient\u0026#34;, \u0026#34;cognito-idp:DeleteUserPoolClient\u0026#34;, \u0026#34;cognito-idp:DescribeUserPoolClient\u0026#34;, \u0026#34;cognito-idp:UpdateUserPoolClient\u0026#34;, \u0026#34;cognito-idp:ListUserPoolClients\u0026#34;, \u0026#34;cognito-idp:CreateUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:DeleteUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:DescribeUserPoolDomain\u0026#34;, \u0026#34;cognito-idp:AdminCreateUser\u0026#34;, \u0026#34;cognito-idp:AdminDeleteUser\u0026#34;, \u0026#34;cognito-idp:AdminGetUser\u0026#34;, \u0026#34;cognito-idp:ListUsers\u0026#34;, \u0026#34;cognito-identity:CreateIdentityPool\u0026#34;, \u0026#34;cognito-identity:DeleteIdentityPool\u0026#34;, \u0026#34;cognito-identity:DescribeIdentityPool\u0026#34;, \u0026#34;cognito-identity:UpdateIdentityPool\u0026#34;, \u0026#34;cognito-identity:ListIdentityPools\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;APIGatewayFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;apigateway:POST\u0026#34;, \u0026#34;apigateway:GET\u0026#34;, \u0026#34;apigateway:PUT\u0026#34;, \u0026#34;apigateway:PATCH\u0026#34;, \u0026#34;apigateway:DELETE\u0026#34;, \u0026#34;apigateway:UpdateRestApiPolicy\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetFunctionConfiguration\u0026#34;, \u0026#34;lambda:ListFunctions\u0026#34;, \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:UpdateFunctionConfiguration\u0026#34;, \u0026#34;lambda:PublishVersion\u0026#34;, \u0026#34;lambda:CreateAlias\u0026#34;, \u0026#34;lambda:UpdateAlias\u0026#34;, \u0026#34;lambda:DeleteAlias\u0026#34;, \u0026#34;lambda:GetAlias\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:AddPermission\u0026#34;, \u0026#34;lambda:RemovePermission\u0026#34;, \u0026#34;lambda:GetPolicy\u0026#34;, \u0026#34;lambda:PutFunctionConcurrency\u0026#34;, \u0026#34;lambda:DeleteFunctionConcurrency\u0026#34;, \u0026#34;lambda:TagResource\u0026#34;, \u0026#34;lambda:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBFullAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:CreateTable\u0026#34;, \u0026#34;dynamodb:DeleteTable\u0026#34;, \u0026#34;dynamodb:DescribeTable\u0026#34;, \u0026#34;dynamodb:ListTables\u0026#34;, \u0026#34;dynamodb:UpdateTable\u0026#34;, \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:DeleteItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;, \u0026#34;dynamodb:Scan\u0026#34;, \u0026#34;dynamodb:BatchGetItem\u0026#34;, \u0026#34;dynamodb:BatchWriteItem\u0026#34;, \u0026#34;dynamodb:DescribeTimeToLive\u0026#34;, \u0026#34;dynamodb:UpdateTimeToLive\u0026#34;, \u0026#34;dynamodb:DescribeContinuousBackups\u0026#34;, \u0026#34;dynamodb:UpdateContinuousBackups\u0026#34;, \u0026#34;dynamodb:TagResource\u0026#34;, \u0026#34;dynamodb:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;KMSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateKey\u0026#34;, \u0026#34;kms:CreateAlias\u0026#34;, \u0026#34;kms:DeleteAlias\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34;, \u0026#34;kms:ListKeys\u0026#34;, \u0026#34;kms:ListAliases\u0026#34;, \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:GenerateDataKey\u0026#34;, \u0026#34;kms:PutKeyPolicy\u0026#34;, \u0026#34;kms:GetKeyPolicy\u0026#34;, \u0026#34;kms:EnableKey\u0026#34;, \u0026#34;kms:DisableKey\u0026#34;, \u0026#34;kms:ScheduleKeyDeletion\u0026#34;, \u0026#34;kms:CancelKeyDeletion\u0026#34;, \u0026#34;kms:TagResource\u0026#34;, \u0026#34;kms:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SecretsManagerAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:PutSecretValue\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodePipelineAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codepipeline:CreatePipeline\u0026#34;, \u0026#34;codepipeline:DeletePipeline\u0026#34;, \u0026#34;codepipeline:GetPipeline\u0026#34;, \u0026#34;codepipeline:GetPipelineState\u0026#34;, \u0026#34;codepipeline:UpdatePipeline\u0026#34;, \u0026#34;codepipeline:ListPipelines\u0026#34;, \u0026#34;codepipeline:StartPipelineExecution\u0026#34;, \u0026#34;codepipeline:StopPipelineExecution\u0026#34;, \u0026#34;codepipeline:GetPipelineExecution\u0026#34;, \u0026#34;codepipeline:ListPipelineExecutions\u0026#34;, \u0026#34;codepipeline:TagResource\u0026#34;, \u0026#34;codepipeline:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodeBuildAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codebuild:CreateProject\u0026#34;, \u0026#34;codebuild:DeleteProject\u0026#34;, \u0026#34;codebuild:UpdateProject\u0026#34;, \u0026#34;codebuild:BatchGetProjects\u0026#34;, \u0026#34;codebuild:ListProjects\u0026#34;, \u0026#34;codebuild:StartBuild\u0026#34;, \u0026#34;codebuild:StopBuild\u0026#34;, \u0026#34;codebuild:BatchGetBuilds\u0026#34;, \u0026#34;codebuild:ListBuildsForProject\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CodeGuruReviewerAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;codeguru-reviewer:AssociateRepository\u0026#34;, \u0026#34;codeguru-reviewer:DescribeRepositoryAssociation\u0026#34;, \u0026#34;codeguru-reviewer:ListRepositoryAssociations\u0026#34;, \u0026#34;codeguru-reviewer:DisassociateRepository\u0026#34;, \u0026#34;codeguru-reviewer:DescribeCodeReview\u0026#34;, \u0026#34;codeguru-reviewer:ListCodeReviews\u0026#34;, \u0026#34;codeguru-reviewer:ListRecommendations\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFormationAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:CreateStack\u0026#34;, \u0026#34;cloudformation:DeleteStack\u0026#34;, \u0026#34;cloudformation:DescribeStacks\u0026#34;, \u0026#34;cloudformation:UpdateStack\u0026#34;, \u0026#34;cloudformation:ListStacks\u0026#34;, \u0026#34;cloudformation:GetTemplate\u0026#34;, \u0026#34;cloudformation:ValidateTemplate\u0026#34;, \u0026#34;cloudformation:DescribeStackEvents\u0026#34;, \u0026#34;cloudformation:DescribeStackResources\u0026#34;, \u0026#34;cloudformation:ListStackResources\u0026#34;, \u0026#34;cloudformation:CreateChangeSet\u0026#34;, \u0026#34;cloudformation:DeleteChangeSet\u0026#34;, \u0026#34;cloudformation:DescribeChangeSet\u0026#34;, \u0026#34;cloudformation:ExecuteChangeSet\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchLogsAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:DeleteLogStream\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:GetLogEvents\u0026#34;, \u0026#34;logs:FilterLogEvents\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;logs:DeleteRetentionPolicy\u0026#34;, \u0026#34;logs:TagLogGroup\u0026#34;, \u0026#34;logs:UntagLogGroup\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;XRayAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;xray:PutTraceSegments\u0026#34;, \u0026#34;xray:PutTelemetryRecords\u0026#34;, \u0026#34;xray:GetSamplingRules\u0026#34;, \u0026#34;xray:GetSamplingTargets\u0026#34;, \u0026#34;xray:GetServiceGraph\u0026#34;, \u0026#34;xray:GetTraceSummaries\u0026#34;, \u0026#34;xray:GetTraceGraph\u0026#34;, \u0026#34;xray:BatchGetTraces\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SNSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sns:CreateTopic\u0026#34;, \u0026#34;sns:DeleteTopic\u0026#34;, \u0026#34;sns:GetTopicAttributes\u0026#34;, \u0026#34;sns:SetTopicAttributes\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;sns:Subscribe\u0026#34;, \u0026#34;sns:Unsubscribe\u0026#34;, \u0026#34;sns:ListSubscriptions\u0026#34;, \u0026#34;sns:ListSubscriptionsByTopic\u0026#34;, \u0026#34;sns:Publish\u0026#34;, \u0026#34;sns:TagResource\u0026#34;, \u0026#34;sns:UntagResource\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMPassRoleForServices\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:PassRole\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;iam:PassedToService\u0026#34;: [ \u0026#34;lambda.amazonaws.com\u0026#34;, \u0026#34;apigateway.amazonaws.com\u0026#34;, \u0026#34;codepipeline.amazonaws.com\u0026#34;, \u0026#34;codebuild.amazonaws.com\u0026#34;, \u0026#34;cloudformation.amazonaws.com\u0026#34; ] } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;IAMRoleManagement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:UpdateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListRolePolicies\u0026#34;, \u0026#34;iam:ListAttachedRolePolicies\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"AWS Cloud Mastery Series #3 – Well-Architected Security Pillar 1. Event Information Event Name: AWS Cloud Mastery Series #3 – Well-Architected Security Pillar Time: 8:30 – 12:00 Organizer: Amazon Web Services (AWS) 2. Purpose of Participation I attended this event to gain a deep understanding of the 5 pillars of the AWS Well-Architected Security Framework, learn about modern cloud security threats, explore end-to-end security implementation from identity to incident response, and apply these principles to real-world enterprise environments.\n3. Agenda Summary 8:30 – 8:50 — Opening \u0026amp; Security Foundation Main content:\nRole of the Security Pillar in the Well-Architected Framework Core principles: Least Privilege – Zero Trust – Defense in Depth Understanding the Shared Responsibility Model Common cloud security threats in Vietnamese enterprises Key insight:\nGained a strong understanding of AWS’s security philosophy, how shared responsibility applies across service layers, and key risks in modern enterprise cloud environments.\n8:50 – 9:30 — Pillar 1: Identity \u0026amp; Access Management Main content:\nModern IAM architecture: Users, Roles, Policies Eliminating long-term credentials; using temporary access IAM Identity Center: SSO and permission sets SCPs \u0026amp; Permission Boundaries for multi-account governance MFA, credential rotation, Access Analyzer Mini Demo: IAM policy validation and access simulation Key insight:\nMastered modern IAM patterns, understood why long-term credentials must be removed, and learned governance models for multi-account environments.\n9:30 – 9:55 — Pillar 2: Detection \u0026amp; Continuous Monitoring Main content:\nOrganization-wide CloudTrail GuardDuty threat detection and Security Hub aggregation Full logging coverage: VPC Flow Logs, ALB logs, S3 access logs Alerting and automation via EventBridge “Detection-as-Code” security automation principles Key insight:\nLearned how to build a holistic monitoring strategy, automate threat detection and response, and treat detection rules as code.\n9:55 – 10:10 — Coffee Break Networking with AWS security specialists and attendees.\n10:10 – 10:40 — Pillar 3: Infrastructure Protection Main content:\nVPC segmentation and network isolation strategies Public vs private subnet best practices Security Groups vs NACLs Integration of WAF + Shield + Network Firewall Workload protection for EC2 and ECS/EKS environments Key insight:\nUnderstood layered network security, when to use each control, and how to secure compute and containerized workloads.\n10:40 – 11:10 — Pillar 4: Data Protection Main content:\nKMS: key policies, grants, rotation Encryption at rest \u0026amp; in transit for S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store rotation patterns Data classification and guardrail implementation Key insight:\nDeep understanding of encryption services, secrets rotation mechanisms, and enterprise-grade data protection strategy.\n11:10 – 11:40 — Pillar 5: Incident Response Main content:\nAWS Incident Response lifecycle Practical playbooks: Compromised IAM credentials S3 public exposure Malware detection on EC2 Snapshot, isolation, and evidence collection Automated IR workflows with Lambda \u0026amp; Step Functions Key insight:\nGained practical IR experience, learned automation patterns, and understood how to orchestrate faster incident remediation.\n11:40 – 12:00 — Wrap-Up \u0026amp; Q\u0026amp;A Main content:\nSummary of all 5 security pillars Common enterprise challenges in Vietnam Certification roadmap: Security Specialty \u0026amp; SA Pro Key Takeaways Clear understanding of all 5 Well-Architected Security Pillars and how they interconnect Strong mastery of modern IAM and Zero-Trust access control Ability to implement comprehensive monitoring and automated detection Knowledge of multi-layer network and workload protection Deep understanding of data encryption and secrets management Practical experience with real enterprise security scenarios Practical Application Perform security assessments using the Well-Architected Security framework Redesign IAM architecture and eliminate long-term credentials Deploy automated detection systems using GuardDuty, Security Hub, EventBridge Implement network segmentation and workload protection Establish encryption \u0026amp; data classification standards Develop automated incident response playbooks Event Experience Participating in “AWS Cloud Mastery Series #3” provided a comprehensive and practical perspective on cloud security.\nThe hands-on demos, clear explanations, and real-world scenarios helped me understand how enterprises implement security at scale.\nHighlights Expert-Led Sessions: Delivered by experienced AWS security specialists Hands-On Demonstrations: IAM policy validation, threat detection workflows, incident response Real Case Studies: Practical insights from real security incidents Complete Coverage: All 5 security pillars explained with implementation guidance Important Lessons Security is a continuous process requiring active monitoring Zero-Trust architecture fundamentally changes IAM strategy Automation is essential to scale security operations Data must be protected at every layer—from infrastructure to application Event Images "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Blog 1 - How Patronus AI Helps Businesses Enhance Reliability When Using Generative AI This blog explains how Patronus AI helps companies increase trust and reliability when deploying Generative AI systems. It highlights the challenges enterprises face—such as hallucinations, safety risks, lack of transparency, and difficulty evaluating LLM performance—and introduces Patronus AI’s automated evaluation platform designed to solve these problems.\nThe article describes how Patronus uses adversarial test generation, automated scoring, benchmarking (such as the FinanceBench dataset), and explainability features to help organizations detect model weaknesses, reduce errors, and ensure safe, compliant AI outputs. It also shows how Patronus integrates with AWS services (EKS, SQS, EC2) to build scalable evaluation workflows.\nOverall, the blog emphasizes the importance of robust AI testing, standardizing evaluation frameworks, improving explainability, and building enterprise confidence in Generative AI applications.\nBlog 2 - Overcoming the Challenges of Kafka Connect with Amazon Data Firehose This blog explains the operational and scaling challenges that customers face when using Kafka Connect to move streaming data from Amazon MSK to downstream systems like Amazon S3. It describes issues such as over/under-provisioning Kafka Connect clusters, fluctuating throughput, operational overhead (SDLC, error handling, retries), and the difficulty of managing connectors reliably at scale.\nThe article then introduces Amazon Data Firehose integration with Amazon MSK as a fully managed, serverless alternative. With this integration, customers no longer need to build or operate their own Kafka Connect consumers—Firehose handles provisioning, scaling, retries, transformations (e.g., JSON to Parquet/ORC), and delivers data directly to S3. A key feature highlighted is the ability to choose the starting offset: from stream creation time, from the earliest data, or from a custom timestamp.\nFinally, the post walks through two migration scenarios: moving from Kafka Connect to Firehose and creating a brand-new MSK → Firehose → S3 pipeline. It shows how using a custom timestamp helps minimize data loss and duplicates during migration, and concludes that Firehose offers a simpler, more cost-efficient, and low-latency way to stream MSK data into S3 for analytics.\nBlog 3 – How Brisa Robotics Uses AWS to Improve Robotics Operations This blog explains how Brisa Robotics transforms traditional material-handling equipment into smart, data-driven autonomous fleets using AWS services. By deploying AWS IoT Greengrass on robots, Brisa collects sensor data, streams it to Amazon Kinesis and Amazon S3, processes it using AWS Lambda, and stores it in Amazon Timestream for real-time analytics.\nThe article highlights how Brisa built a flexible, offline-capable data pipeline that adapts to different customer environments without requiring infrastructure changes. Using this architecture, Brisa provides dashboards that visualize robot positions, heatmaps, camera imagery, and operational KPIs—helping customers improve warehouse safety, efficiency, and decision-making.\nThis AWS-powered solution enabled Brisa’s client (a major global brewery) to enhance accuracy, increase data collection frequency, identify bottlenecks, and improve warehouse operations—all without modifying their existing workflows.\n"},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Learn about Amazon Elastic Compute Cloud and the different instance types. Understand several services and resource management mechanisms on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study the general theory of EC2 concepts and applications, along with the services required to set up a fully functional EC2 instance. 09/22/2025 09/22/2025 3 - Learn about the concepts and applications of AWS Backup. - Practice: + Create S3 Bucket + Deploy Infrastructure + Create Backup plan + Set up notifications + Test Restore 09/23/2025 09/23/2025 https://000013.awsstudygroup.com/ 4 - Learn about the AWS Storage Gateway Practice: + Create S3 Bucket + Create EC2 Storage Gateway + Create Storage Gateway + Create File Shares + Mounting File Shares on On-premise Machine 09/24/2025 09/24/2025 https://000024.awsstudygroup.com/ 5 - Learn basic Amazon Simple Storage Service (Amazon S3): + Durability \u0026amp; Performance + Availability + Storage Classes + Security + Scalability 08/25/2025 08/25/2025 https://000057.awsstudygroup.com/ 6 - Review lessons from the previous days (EC2, AWS Backup, Storage Gateway, S3) - Redo all practice exercises to strengthen understanding - Summarize key concepts learned during the week 09/26/2025 09/26/2025 Week 3 Achievements: Gained a basic understanding of EC2 concepts and instance types. Successfully practiced setting up EC2 and related services. Learned the fundamental concepts and usage of AWS Backup and completed backup–restore exercises. Understood how AWS Storage Gateway works and practiced creating file shares. Learned core knowledge of Amazon S3, including storage classes, durability, and security. Reviewed and reinforced all lessons through repeated practice and summary activities. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.8-cognito/5.8.3-email-verification/","title":"Setup Email Verification","tags":[],"description":"","content":"Setup Email Verification In this step, you will configure email verification to ensure users verify their email addresses during registration.\nNavigate to your User Pool in the Cognito console Go to Sign-up experience tab Click Edit in the Attribute verification and user account confirmation section Configure Email Verification Attribute verification and user account confirmation:\n✅ Send email message, verify email address Verification message: Code Email verification subject: Verify your email for Task Management System Email verification message: Your verification code for Task Management System is {####}. Please enter this code to complete your registration. Configure Email Delivery Email delivery method:\nSend email with Cognito (for development) FROM email address: no-reply@verificationemail.com Note: For production applications, consider using Amazon SES for better email deliverability and custom domains.\n"},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Vietnam Cloud Day 2025 1. Event Information Event Name: Vietnam Cloud Day 2025 Time: 7:35 – 16:30 Organizer: Amazon Web Services (AWS) 2. Purpose of Participation I attended the event to update myself on the latest trends in Cloud Computing, Generative AI (GenAI), and how AWS is building the technical, data, and security foundations that enable organizations to adopt AI effectively.\nAdditionally, I wanted to understand how business leaders shape AI strategy, contributing to my own career direction.\n3. Agenda Summary 7:35 – 9:00 — Check-in Participants completed check-in and interacted before the official program began.\n9:00 – 9:20 — Opening Opening remarks by a Government representative, emphasizing the role of digital technology and AI in national digital transformation strategies.\n9:20 – 9:40 — Keynote Address Eric Yeo – Country General Manager, AWS Vietnam, Cambodia, Laos \u0026amp; Myanmar\nKey points:\nAWS’s long-term investment commitment in Vietnam Accelerating business growth through Cloud \u0026amp; GenAI Strong regional technology talent potential 9:40 – 10:00 — Customer Keynote 1 Dr. Jens Lottner – CEO, Techcombank\nHighlights:\n“AI-first” strategy in the banking sector Building internal data and AI capabilities Vision to become a technology-led bank 10:00 – 10:20 — Customer Keynote 2 Ms. Trang Phung – CEO \u0026amp; Co-Founder, U2U Network\nContent:\nCombining Blockchain \u0026amp; AI to create innovative products Building an emerging Web3 ecosystem Developing a startup community 10:20 – 10:50 — AWS Keynote Jaime Valles – VP \u0026amp; GM, Asia Pacific \u0026amp; Japan, AWS\nFocused on:\nGenAI as the core of next-generation applications AWS’s comprehensive AI service ecosystem Sustainable and secure technology development models 11:00 – 11:40 — Panel Discussion Navigating the GenAI Revolution: Strategies for Executive Leadership\nModerator: Jeff Johnson (AWS)\nPanelists:\nVu Van – CEO, ELSA Corp Nguyen Hoa Binh – Chairman, NextTech Dieter Botha – CEO, TymeX Key insights:\nExecutive leadership thinking in the GenAI era Building a culture of innovation and embracing change Aligning AI initiatives with business objectives instead of chasing trends 13:30 – 14:00 — Building a Unified Data Foundation on AWS Kien Nguyen – Solutions Architect, AWS\nStrategies for building a unified data foundation Ingestion → Storage → Processing → Governance Preparing data for large-scale Analytics \u0026amp; AI 14:00 – 14:30 — GenAI Adoption \u0026amp; Roadmap on AWS Jun Kai Loke \u0026amp; Tamelly Lim – AWS\nContent:\nGlobal GenAI trends AWS roadmap: from infrastructure to model layers Key AI services: Bedrock, SageMaker, Agents… 14:30 – 15:00 — AI-Driven Development Lifecycle (AI-DLC) Binh Tran – Senior Solutions Architect, AWS\nNew SDLC model with AI as a central collaborator AI-driven execution, human supervision Accelerated development and improved product quality 15:30 – 16:00 — Securing Generative AI Applications on AWS Taiki Dang – Solutions Architect, AWS\nSecurity challenges across three layers: Infrastructure – Model – Application Zero-trust architecture, IAM, encryption by default Protecting data throughout the AI lifecycle 16:00 – 16:30 — Beyond Automation: AI Agents as Productivity Multipliers Michael Armentano – Principal WW GTM Specialist, AWS\nAI Agents becoming “digital employees” Self-learning, adaptive, and autonomous execution Future productivity multiplied by AI Key Takeaways AWS GenAI strategy from leadership to technical perspectives AI-DLC as the future of software development Data architecture required for AI/ML Multi-layer security for Generative AI Potential of AI Agents in enterprises Strategic thinking in AI and the ability to connect Cloud – Data – AI Practical Application Applying AI-DLC concepts into software development and DevOps Connecting lessons in data \u0026amp; infrastructure to AI/ML projects Career direction aligned with Cloud \u0026amp; AI development Preparing necessary skills for real-world AI/ML projects Applying multi-layer security knowledge in system design Event Experience The event provided a comprehensive perspective on GenAI, Cloud, and enterprise AI deployment strategies.\nKeynotes from AWS executives and business leaders helped me better understand how AI is operated in real environments and the infrastructure, data, and security requirements involved.\nHighlights Content: Comprehensive, high quality, and fully updated by AWS Speakers: Senior AWS experts and leaders from top enterprises Organization: Professional and seamless Value: Deep insights and strong strategic direction for Cloud \u0026amp; AI Important Lessons GenAI requires a strong data and security foundation to be successful The AI-DLC model transforms traditional software development AI Agents will become essential for enterprise productivity AI implementation must align with business objectives and system architecture Event Images "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Learn about VM Import/Export and related storage services. Practice managing AWS resources using the Console and CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about VM Import/ Export - Practice: + Export Virtual Machine from On-premises + Upload virtual machine to AWS + Import virtual machine to AWS + Deploy Instance from AMI 09/29/2025 09/29/2025 https://000014.awsstudygroup.com/ 3 - Export instance from AWS - Practice: + Setting up S3 bucket ACL + Export virtual machine from Instance + Export virtual machine from AMI 09/30/2025 09/30/2025 https://000014.awsstudygroup.com/ 4 - Learn about Amazon FSx for Windows File Server - Practice: + Create Environment + Create an SSD Multi-AZ file system + Create an HDD Multi-AZ file system + Create new file shares + Test Performance 10/01/2025 10/01/2025 https://000025.awsstudygroup.com/ 5 - Practice: + Monitor Performance + Enable data deduplication + Enable shadow copies + Manage user sessions and open files 10/02/2025 10/02/2025 https://000025.awsstudygroup.com/ 6 - Practice: + Enable user storage quotas + Enable Continuous Access share　+ Scale throughput capacity　+ Scale storage capacity + Delete environment 10/03/2025 10/03/2025 https://000025.awsstudygroup.com/ Week 4 Achievements: Learned and practiced VM Import/Export. Exported and imported virtual machines using S3 and AMI. Worked with Amazon FSx for Windows File Server and created file systems. Practiced file sharing, performance testing, and FSx management features. Completed scaling tasks and cleaned up the environment. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.4-apigateway/","title":"API Gateway","tags":[],"description":"","content":"Initializing API Gateway Create an API Gateway Function Open API Gateway\nSelect Create API Choose API type: REST API\nThen select: Build\nConfigure API Information Enter API name, for example: taskhub-backend-api\nEndpoint type: Regional\nSecurity policy: SecurityPolicy_TLS13_1_3_2025_09\nClick Create API\nCreate Resource for API In the API Gateway menu, select Resources\nClick Create resource\nEnter:\nResource name: auth, task, projects … depending on the project Resource path: /auth, /task, … Click Create resource\nCreate Method and Connect to Lambda Select a Resource → click Create method\nChoose ANY (or POST, GET depending on your API) Integration type: Lambda Function Tick Use Lambda Proxy integration Select Region Enter the name of the Lambda function, e.g., taskhub-backend_1 Click Save\nRepeat these steps to create additional API routes.\nGrant API Gateway Permission to Call Lambda Choose Deploy API\nCreate a new stage (if not available): prod1 Click Deploy Result: You will receive an Invoke URL like:\nhttps://ne6pw5hqej.execute-api.ap-southeast-1.amazonaws.com/prod1\n"},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.8-cognito/5.8.4-app-client/","title":"Configure App Client","tags":[],"description":"","content":"Configure App Client When creating the User Pool, AWS Cognito automatically created a default App Client. In this step, we will configure this App Client to suit our application needs.\nNavigate to your User Pool in the Cognito console Go to App integration tab You will see the App Client has already been created Edit App Client Click on the App Client name to edit it Or click Edit if available Update app client information:\nApp client name: TaskManagementWebApp (if you want to change it) App client type: Public client Authentication flows: ✅ ALLOW_USER_PASSWORD_AUTH ✅ ALLOW_REFRESH_TOKEN_AUTH ✅ ALLOW_USER_SRP_AUTH Configure Hosted UI Hosted UI settings:\nUse the Cognito Hosted UI: Enabled Domain type: Use a Cognito domain Cognito domain: taskmanagement-auth-[your-unique-id] Initial app client settings:\nAllowed callback URLs: http://localhost:3000/callback https://your-app-domain.com/callback Allowed sign-out URLs: http://localhost:3000/ https://your-app-domain.com/ OAuth 2.0 settings:\nAllowed OAuth flows: ✅ Authorization code grant ✅ Implicit grant Allowed OAuth scopes: ✅ email ✅ openid ✅ profile Save Configuration Review all configurations Click Save changes Verify App Client Configuration After updating, note down the important information:\nApp Client Details:\nClient ID: [your-client-id] Hosted UI URL: https://taskmanagement-auth-[your-id].auth.us-east-1.amazoncognito.com The App Client is now configured and ready to handle authentication requests from your application. You can use the Client ID and Hosted UI URL to integrate with your web or mobile application.\n"},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/4-eventparticipated/","title":"Participated Events","tags":[],"description":"","content":"Event 1 Event Name: AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS\nTime: 08:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu, District 1, Ho Chi Minh City\nRole: Participant\nContent: The workshop introduced an overview of AI/ML/GenAI on AWS, hands-on practice of the Machine Learning lifecycle with SageMaker, building a GenAI Chatbot using Amazon Bedrock, and applying Prompt Engineering, RAG, Agents, and Guardrails.\nLessons: GenAI requires a complete workflow, RAG is the foundation for enterprise chatbots, SageMaker standardizes the ML lifecycle, and choosing the right Foundation Model helps optimize performance and cost.\nEvent 2 Event Name: AWS Cloud Mastery Series #2 – DevOps on AWS\nTime: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu, District 1, Ho Chi Minh City\nRole: Participant\nContent: Learned DevOps Mindset, built CI/CD pipelines with CodeCommit, CodeBuild, CodeDeploy, and CodePipeline; implemented infrastructure using CloudFormation \u0026amp; CDK; used Containers (ECR, ECS, EKS); and set up Monitoring with CloudWatch \u0026amp; X-Ray.\nLessons: DevOps is a culture of continuous improvement, IaC is the foundation for sustainable infrastructure management, Containers are the future of application deployment, and Monitoring is critical for stable system operations.\nEvent 3 Event Name: AWS Cloud Mastery Series #3 – Well-Architected Security Pillar\nTime: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu, District 1, Ho Chi Minh City\nRole: Participant\nContent: Introduced the 5 Well-Architected Security pillars: IAM, Detection \u0026amp; Monitoring, Infrastructure Protection, Data Protection, and Incident Response; guided the implementation of end-to-end security from identity, network, and data protection to automated incident response using AWS services such as IAM, GuardDuty, KMS, WAF, CloudWatch, and Lambda.\nLessons: Security is a continuous process, Zero Trust must be applied to IAM, monitoring and automation are essential for fast incident response, and data must be protected at all layers from infrastructure to application.\nEvent 4 Event Name: AWS Vietnam Cloud Day 2025\nTime: 09:00 – 17:30, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu, District 1, Ho Chi Minh City\nRole: Participant\nContent: Updated AWS GenAI strategy, data platform architecture for AI/ML, AI-DLC software development model, multi-layer security for Generative AI, and the application potential of AI Agents in enterprises.\nLessons: GenAI requires strong data and security foundations, AI-DLC changes the traditional software development approach, AI Agents will significantly boost productivity, and AI adoption must be aligned with business objectives.\nEvent 5 Event Name: AWS GameDay - Generative AI Unicorn Party\nTime: 13:30 – 17:00, November 14, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu, District 1, Ho Chi Minh City\nRole: Participant\nContent: Hands-on experience with AWS GameDay using Amazon Bedrock, Knowledge Base, and AI Agents through team-based challenges, combining quizzes and real-time practice on the AWS platform.\nLessons: Learned to apply Bedrock and Knowledge Base to build AI Chatbots, while developing problem-solving skills and teamwork in a real AI environment.\nEvent 6 Event Name: Data Science on AWS\nTime: 09:30 – 11:45, October 16, 2025\nLocation: FPT University, Ho Chi Minh City Campus\nRole: Participant\nContent: Introduced the Data Science Pipeline on AWS (S3 – Glue – SageMaker), demonstrated data processing with Glue, trained and deployed a Sentiment Analysis model using SageMaker, and discussed Cloud vs On-premise Data Science.\nLessons: Cloud plays a core role in modern Data Science systems, integrating S3 – Glue – SageMaker enables a complete pipeline, and hands-on practice is essential to mastering Machine Learning.\n"},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a Task Management Platform with DevOps on AWS Serverless Overview AWS Serverless enables you to build and deploy applications without managing servers, automatically scales based on demand, and you only pay for what you use.\nIn this workshop, we will learn how to design, build, and deploy a complete task management platform TaskHub using serverless architecture and automated DevSecOps practices.\nWe will create a system that includes frontend, backend API, database, and a complete CI/CD pipeline. The workshop focuses on three main components to build a production-ready application on AWS:\nServerless Backend - Use AWS Lambda for business logic processing, API Gateway as the communication layer, DynamoDB for data storage, and Cognito for user authentication management with optimized costs.\nContent Delivery - Deploy Next.js application on S3, distribute globally via CloudFront with low latency, and protect with AWS WAF against common web attacks.\nDevOps Pipeline - Automate the build, test, and deploy process using CodePipeline and CodeBuild, integrate security scanning with CodeGuru, and manage infrastructure as code with CloudFormation.\nContent Workshop overview Prerequiste Deploying Serverless Functions with AWS Lambda Building an API Gateway with Amazon API Gateway Simple and Secure Object Storage with Amazon S3 Accelerating Content Delivery with Amazon CloudFront (CDN) Managing User Identity and Access with Amazon Cognito Managing Encryption Keys with AWS Key Management Service (KMS) SecretManager WAF "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"AWS Gameday - Generated AI Unicorn Party 1. Event Information Event Name: AWS Gameday - Generated AI Unicorn Party Time: 13:30 – 17:00 Organizer: Amazon Web Services (AWS) 2. Purpose of Participation I attended the event with the desire to learn more about AWS AI services such as Bedrock and to participate in the game in order to gain hands-on experience with the Bedrock service.\n3. Agenda Summary 14:30 – 15:00 — Introduction to AWS Gamedays Main content:\nIntroduction to Gameday and AI services such as Bedrock 15:00 – 15:20 — Guidance for Event Participants Main content:\nParticipants communicated and discussed with each other to form teams The speakers guided participants on how to set up the environment to experience the Gamedays 15:20 – 16:40 — The Game Begins Main content:\nTeams started accessing the game Answered questions and practiced the services required by the game in order to earn points for their teams 16:40 – 17:00 — Game Conclusion Main content:\nAwards were given to the top 3 teams with the highest scores Key Takeaways How to use Bedrock and how to train AI, as well as how to use the Knowledge Base in AI Agents Practical Applications How to build an AI chatbot that can answer questions based on the provided Knowledge Base Event Experience Participating in “AWS Gameday - Generated AI Unicorn Party” provided a comprehensive perspective on the Bedrock service and how it is used. With well-structured labs and a competitive team-based scoring environment, the event created strong motivation and allowed for faster knowledge acquisition, supported by clear and focused guidance from the speakers through hands-on game-based exercises.\nHighlights In-depth training: Presented by experienced AWS experts Real-world case studies: Reinforced knowledge through practical questions in the game Comprehensive knowledge: Provided a large amount of insights into how an AI chatbot operates based on a Knowledge Base and AWS AI Models Important Lessons In real enterprise environments, the application of AI Agents is becoming increasingly common, requiring solid knowledge of AI Models for development Practice through real-world lessons and hands-on training on the AWS platform Event Images "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Learn Security Hub, cost-optimization with Lambda, and resource management using Tags and Resource Groups. Understand tag-based EC2 access control and the concept of IAM Permission Boundaries. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about the AWS Security Hub - Practice: + Enable Security Hub + Score for each set of criteria 10/06/2025 10/06/2025 https://000018.awsstudygroup.com/ 3 - Understand how to the Lambda functions to enhance cost efficiency within the AWS environment. - Practice: + Create Tag for Instance + Create Role for Lambda + Create Lambda Function + Check Result 10/07/2025 10/07/2025 https://000022.awsstudygroup.com/ 4 - Understanding how to Manage Resources Using Tags and Resource Groups - Practice: + Create EC2 Instance with tag + Managing Tags in AWS Resources + Filter resources by tag + Using tags with CLI + Create a Resource Group 10/08/2025 10/08/2025 https://000027.awsstudygroup.com/ 5 - Manage access to ec2 services with resource tags through iam services - Practice: + Create IAM user + Create IAM Policy + Create IAM Role + Check Policy + Switch Roles + Check IAM Policy 10/09/2025 10/09/2025 https://000028.awsstudygroup.com/ 6 - Learn about IAM Permission Boundary - Practice: + Create Limit Policy + Create IAM Limited User + Check IAM User Limit 10/10/2025 10/10/2025 https://000030.awsstudygroup.com/ Week 5 Achievements: Learn how to use AWS Security Hub to monitor and evaluate security posture. Understand how Lambda can automate cost-optimization tasks. Learn to manage AWS resources using Tags and Resource Groups. Manage access control for EC2 using resource tags with IAM policies. Understand IAM Permission Boundaries and how to apply them. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.6-s3/","title":"Set up S3","tags":[],"description":"","content":"WORKLOG: S3 ORIGIN CONFIGURATION (FULL) This Worklog outlines the steps for creating and configuring two (02) separate Amazon S3 Buckets, serving as the Origins for different system resources: Frontend Code and user-uploaded files.\n1. S3 Bucket 1 Configuration: taskhub-frontend-prod (Frontend Origin) This Bucket serves as the Origin for CloudFront, storing Frontend code (HTML, CSS, JS) and static assets.\n1.1. Access and Bucket Creation Log in to the AWS Console, find and select the Amazon S3 service. Click the \u0026ldquo;Create bucket\u0026rdquo; button. Fill in the general configuration information: Configuration Value Explanation Bucket name taskhub-frontend-prod The Bucket name must be globally unique. AWS Region Asia Pacific (Singapore) ap-southeast-1 Select the geographical region closest to the target users to optimize latency. 1.2. Object Ownership \u0026amp; Public Access Configuration Object Ownership: Select ACLs disabled (recommended). Purpose: Access permissions are managed centrally using a Bucket Policy, simplifying management. Block Public Access settings for this bucket: Action: Uncheck [ ] Block all public access (and all sub-options). Reason: This allows us to configure a Bucket Policy later to grant read-only access specifically to CloudFront OAC (Origin Access Control), ensuring S3 can function as a valid private Origin. 1.3. Versioning and Encryption Configuration Configuration Value Explanation Bucket Versioning Select: Disable Reduces Storage Costs as maintaining multiple versions of Frontend code is unnecessary. Default encryption Enable Ensures data is encrypted when stored (at rest). Encryption type Select: Server-side encryption with Amazon S3 managed keys (SSE-S3) The default, cost-effective encryption method. Bucket Key Select: Enable Minimizes Request Costs related to the encryption/decryption process. 2. S3 Bucket 2 Configuration: taskhub-files-prod (User Files Storage) This Bucket is used to store files uploaded by users (images, media\u0026hellip;). Maximum security is prioritized. The creation process is similar to the S3 Bucket above.\n2.1. Access and Bucket Creation Repeat the Bucket creation steps (Section 1.1). Bucket name: taskhub-files-prod AWS Region: Asia Pacific (Singapore) ap-southeast-1 2.2. Public Access Configuration (SECURITY DIFFERENCE) Block Public Access settings for this bucket: Keep [X] Block all public access (CHECK ALL). Reason: This is a Secure Bucket containing user data. Files MUST NOT be publicly accessible. Access will only be temporarily granted via Pre-signed URLs generated by the Backend API after authentication. 2.3. Versioning and Encryption Configuration (Similar) Configuration Value Explanation Bucket Versioning Select: Disable Prevents rapid storage cost increases when users update/delete files. Default encryption Enable Data encryption is mandatory for user data. Encryption type Select: Server-side encryption with Amazon S3 managed keys (SSE-S3) Standard S3 encryption. Bucket Key Select: Enable Reduces encryption request costs. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/4-eventparticipated/4.6-event6/","title":"Event 6","tags":[],"description":"","content":"Workshop – Data Science on AWS 1. Event Information Event Name: Data Science on AWS Time: 09:30 – 11:45, October 16, 2025 Speakers: Kha Van – Cloud Solution Architect at AWS, AWS Community Builder Bach Doan Vuong – DevOps Engineer at FPT, AWS Community Builder 2. Purpose of Participation The purpose of joining this workshop was to learn how to implement Data Science on the AWS platform, from data processing and model training to real-world deployment, as well as to better understand the role of Cloud computing in modern Data Science systems.\n3. Agenda Summary 09:30 – 09:40 | Opening \u0026amp; Goals – Kha Van Main content:\nIntroduction to the workshop and learning objectives Explanation of why Data Science needs Cloud computing 09:40 – 10:05 | Data Science Pipeline on AWS – Kha Van Main content:\nRecap of the Data Science Pipeline: Ingest → Process → Model Mapping to AWS services: Amazon S3 AWS Glue Amazon SageMaker 10:05 – 10:35 | Demo 1 – Data Processing with Glue – Bach Doan Vuong Main content:\nImporting the IMDb dataset Performing: Data cleaning Feature extraction Observing the processed results on Amazon S3 10:35 – 11:00 | Demo 2 – Model Training on SageMaker – Bach Doan Vuong Main content:\nTraining a Sentiment Analysis model Monitoring metrics Deploying an endpoint and testing with sample input 11:00 – 11:35 | Deep Dive \u0026amp; Discussion – Kha Van Main content:\nComparison between Cloud vs On-premise Data Science Analysis of: Performance Cost 11:35 – 11:45 | Homework Project Briefing – Kha Van Main content:\nIntroduction to Workshop Project – Part 1 Design and description of a Data Science Pipeline on AWS using a self-selected dataset Requirements: 2–3 page report + architecture diagram Key Takeaways Clear understanding of the Data Science Pipeline on AWS Practical application of AWS Glue and Amazon SageMaker Practical Applications Building systems for: Data processing Model training Machine Learning API deployment on AWS Event Experience The workshop provided a clear perspective on how a Data Science system operates in a Cloud environment. The combination of theory and hands-on demos made the learning process more effective and helped participants better understand real-world enterprise deployment workflows.\nHighlights Hands-on demonstrations with AWS Glue and Amazon SageMaker Content closely followed a complete Data Science Pipeline on AWS Speakers shared real-world experience from enterprise environments Recommendations for follow-up courses to help students deepen their understanding Important Lessons Understanding the critical role of Cloud computing in modern Data Science systems Learning how to integrate S3 – Glue – SageMaker into a complete pipeline Recognizing the importance of hands-on practice and real-world deployment in Machine Learning learning Event Images "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at [Amazon Web Services Vietnam Co., Ltd.] from [06/09/2025] to [end date], I had the opportunity to learn, practice, and apply the knowledge I acquired at university in a real working environment. I participated in [group discussions to design the project’s infrastructure architecture, analyzing AWS services and optimizing costs, deploying infrastructure components on AWS, configuring project environments, and programming to integrate various AWS services], through which I improved my skills [by learning and collaborating with my team, which strengthened my communication skills and technical knowledge; working directly with AWS services in a real project helped me gain deeper insights into their operations; and the project development process also enhanced my programming abilities and reporting skills].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Learn core AWS security and access-management concepts using KMS and IAM roles. Understand basic database and data-analytics workflows with RDS, Glue, Athena, and QuickSight. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn How to Encrypt at rest with AWS KMS - Practice: + create policy, role, group and user + Create Key Management Service + Create Amazon S3 + Create AWS CloudTrail and Amazon Athena - Practice: Test and share encrypted data on S3 10/13/2025 10/13/2025 https://000033.awsstudygroup.com/ 3 - Granting authorization for an application to access AWS services with an IAM role. - Practice: + Use access key + Create IAM role + using an IAM role assigned to an EC2 instance 10/14/2025 10/14/2025 https://000048.awsstudygroup.com/ 4 - Learn about Amazon Relational Database Service (Amazon RDS) - Practice: + set up the necessary network infrastructure and security components to ensure proper connectivity and security for our database environment. + Create EC2 instance + Create RDS database instance + Application Deployment + Backup and Restore 10/15/2025 10/15/2025 https://000005.awsstudygroup.com/ 5 Understanding the data lake and how to use it - Practice: + Create an IAM role for AWS Glue. + Create S3 Bucket + Creating a Delivery Stream + Create Sample Data 10/16/2025 10/16/2025 https://000035.awsstudygroup.com/ 6 - Practice: + Create Data Catalog + Create SageMaker Notebook + Amazon Athena Overview + Visualize with QuickSight 10/17/2025 10/17/2025 https://000035.awsstudygroup.com/ Week 6 Achievements: Learned how to encrypt data at rest using AWS KMS and reviewed CloudTrail logs. Granted application access to AWS services by using IAM roles on EC2 instances. Set up and managed an Amazon RDS database, including backup and restore. Practiced data lake workflows using AWS Glue, S3, Firehose, Athena, SageMaker Notebook, and QuickSight. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/7-feedback/","title":"Feedback &amp; Suggestions","tags":[],"description":"","content":" Here you can freely share your personal opinions about your experience while participating in the First Cloud Journey program, helping the FCJ team improve any remaining limitations based on the following categories:\nGeneral Evaluation 1. Working Environment\nThe working environment at FCJ is very friendly and comfortable. Everyone is always willing to support me whenever I face difficulties, even outside working hours. The workspace is neatly organized, creating a pleasant atmosphere and helping me stay focused.\n2. Support from Mentor / Admin Team\nMy mentor always provided thorough guidance and clear explanations whenever I was unsure about something, and encouraged me to ask questions to broaden my understanding. The admin team supported me with all necessary procedures and documents and consistently created favorable conditions for my work. What I appreciate most is that my mentor did not immediately give me the answers, but instead encouraged me to try, experiment, and solve the problems on my own, which helped me learn more effectively.\n3. Alignment Between Tasks and Academic Background\nThe tasks assigned to me were closely aligned with the knowledge I learned at university, while also introducing new areas I had not previously encountered. Thanks to that, I strengthened my foundational understanding and acquired additional practical skills.\n4. Opportunities for Learning \u0026amp; Skill Development\nDuring the internship, I learned many important skills such as using project management tools, team collaboration, and professional communication in a corporate environment. My mentor also shared valuable real-world experiences that helped me better orient my future career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company’s working culture is very positive: everyone respects one another, works seriously, yet maintains a cheerful atmosphere. When there were urgent tasks, team members supported each other regardless of their positions. This made me feel welcomed and integrated, even as an intern.\n6. Policies / Benefits for Interns\nThe company provides internship allowances and offers flexible working hours when needed. Additionally, participating in internal training sessions is a significant advantage, helping me expand my knowledge and learn new skills.\nAdditional Questions What did you feel most satisfied with during the internship?\n\u0026ndash;\u0026gt; What I was most satisfied with was the friendly working environment and the supportive team culture. My mentor and team members were always willing to explain, share experiences, and give me opportunities to try working on real tasks.\nWhat do you think the company should improve for future interns?\n\u0026ndash;\u0026gt; In my opinion, the company could organize more internal sharing sessions or activities that help interns connect with each other, which would strengthen teamwork and communication.\nIf you were to refer this program to your friends, would you recommend them to intern here? Why?\n\u0026ndash;\u0026gt; I would definitely encourage my friends to intern here if they have the opportunity. The professional environment, supportive mentors, and practical learning experience make this an ideal place for students to develop their skills and experience corporate culture.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\n\u0026ndash;\u0026gt; During my internship, I noticed that there were not many workshops where the team could practice together and share knowledge. I suggest adding some group activities or mini competitions such as Kahoot quizzes or similar tools to help interns reinforce and remember the company’s service-related knowledge more effectively.\nWould you like to continue participating in this program in the future?\n\u0026ndash;\u0026gt; Absolutely. Personally, being able to intern here has brought me a lot of practical value and knowledge through labs and company events. It helped me understand the industry better and gain clearer career direction. The creative, friendly, and professional environment is a huge motivation for me.\nOther comments (free sharing):\n\u0026ndash;\u0026gt; I was quite nervous on my first day at the company, but fortunately the mentors were very supportive and welcoming. I hope the program continues to maintain this spirit and expand it even further so that students like us can continue to develop both personally and professionally.\n"},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Learn about AWS infrastructure services. Understand how services integrate and operate together optimally. Research potential project topics and begin designing the project architecture. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about KMS, data encryption, and log monitoring 10/20/2025 10/20/2025 3 - Explore how AWS services work together to build team project infrastructure 10/21/2025 10/21/2025 4 - Team discussion and research to select a suitable project topic 10/22/2025 10/22/2025 5 - Finalize the project topic and begin identifying required AWS services 10/23/2025 10/23/2025 6 - Start designing the project infrastructure and consult mentors for feedback 10/24/2025 10/24/2025 Week 7 Achievements: Gained a clear understanding of key AWS infrastructure services such as KMS and IAM, and how they integrate to build a secure and optimized environment. Learned how to combine multiple AWS services to design a foundational architecture for a real project. Collaborated with the team to analyze and choose an appropriate project topic, identifying necessary AWS services. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.7-cloudfront/","title":"Set up CloudFront","tags":[],"description":"","content":"COMPLETE CLOUDFRONT DISTRIBUTION CONFIGURATION This Worklog outlines the steps for configuring a CloudFront Distribution, focusing on setting up Origin Access Control (OAC) security and verifying the prerequisites on the S3 Bucket.\n1. S3 PREREQUISITES CHECK Before finalizing CloudFront setup, you must ensure the S3 Bucket taskhub-frontend-prod is protected and configured correctly.\n1.1. Static Website Hosting Status Check: The Properties tab of the S3 Bucket. Status: S3 static website hosting must be in the Disabled state. Explanation: Since CloudFront acts as the CDN, Static Website Hosting is not required on S3. S3 merely serves as the content repository (Origin). 1.2. Block Public Access \u0026amp; Bucket Policy Check: The Permissions tab of the S3 Bucket. Block public access (bucket settings): Must be in the On state (Block all public access). Bucket policy (Final Confirmation): Must contain the OAC Policy that is automatically updated by CloudFront . 2. CLOUDFRONT DISTRIBUTION CONFIGURATION 2.1. Step 1 \u0026amp; 2: Get started Configuration Value Explanation Action Click \u0026ldquo;Create distribution\u0026rdquo;. Start the CDN creation process. Plan Select Free Plan ($0/month). The free plan for the project. Distribution name taskhub-frontend-cdn A memorable name for the resource. Distribution type Single website or app. The appropriate type for a Frontend application. 2.2. Step 3: Specify origin (OAC Setup) 1. Specify Origin Origin type: Select Amazon S3. S3 origin: Select the S3 Bucket taskhub-frontend-prod. 2. Configure OAC (Automatic Security) Tick \u0026ldquo;Allow private S3 bucket access to CloudFront - Recommended\u0026rdquo;. Select Use recommended origin settings. Explanation: Upon completing the Distribution creation, CloudFront will automatically update the S3 Bucket Policy to grant access via OAC. 3. Cache Configuration Cache settings: Select Use recommended cache settings tailored to serving S3 content. Default Root Object: Enter index.html (Standard configuration for SPA). 2.3. Step 4 \u0026amp; 5: Security \u0026amp; TLS Configuration Applied Value Explanation WAF Uncheck paid features. Keeping defaults for the project. Viewer protocol policy Redirect HTTP to HTTPS Mandates the use of the secure protocol. Custom SSL certificate Default CloudFront Certificate Activates free HTTPS. 2.4. Step 6: Review and create 1. Review Origin (Source Confirmation) S3 origin: taskhub-frontend-prod. (Must ensure the correct Frontend Bucket is selected). Grant CloudFront access to origin: Must be Yes. (Confirms OAC functionality). 2. Final Configuration Price Class: Select Use all edge locations (best performance). Click the \u0026ldquo;Create distribution\u0026rdquo; button to deploy. 3. POST-DEPLOYMENT CHECK: OAC SECURITY CONFIRMATION After the Distribution status changes to Deploying, perform this crucial check to confirm that OAC is functional:\nAccess S3 Bucket taskhub-frontend-prod. Navigate to the \u0026ldquo;Permissions\u0026rdquo; tab. Find the \u0026ldquo;Bucket policy\u0026rdquo; section. Confirm: The Policy must be automatically updated and contain the JSON statement authorizing the CloudFront service. This confirms that OAC has blocked direct access and only allows CloudFront to read the files. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Review fundamental knowledge to prepare for the midterm exam Practice through AWS Certified Cloud Practitioner (CLF-C02-English) exam questions Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Begin reviewing core AWS service knowledge 10/27/2025 10/27/2025 3 - Take online practice exams via Skill Builder 10/28/2025 10/28/2025 4 - Adjust the project topic due to misalignment; discuss with the team to choose a new topic and related AWS services 10/29/2025 10/29/2025 5 - Continue studying for the midterm exam using Skill Builder and YouTube 10/30/2025 10/30/2025 6 - Practice exam questions 10/31/2025 10/31/2025 Week 8 Achievements: Reviewed foundational AWS service knowledge to strengthen preparation for the midterm exam. Completed multiple practice exams on AWS Skill Builder, gaining familiarity with question formats and applying knowledge to practical scenarios. Reassessed the initial project topic with the team, identified misalignment, and selected a more suitable project direction along with necessary AWS services. Continued practicing through Skill Builder and YouTube, improving question-solving speed and analytical skills. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.8-cognito/","title":"Setup AWS Cognito Authentication","tags":[],"description":"","content":"AWS Cognito User Authentication Workshop Overview AWS Cognito provides user identity and access management for web and mobile applications. It enables you to add user sign-up, sign-in, and access control to your applications quickly and easily.\nIn this workshop, you will learn how to:\nCreate and configure a Cognito User Pool Setup password policies and email verification Configure App Client with Hosted UI Implement basic email/password authentication Content Create Cognito User Pool Configure Password Policies Setup Email Verification Configure App Client "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.9-keymanagementservice/","title":"Set up KMS","tags":[],"description":"","content":"Objective Create a Customer Managed Key (CMK) on AWS KMS to:\nEncrypt data in DynamoDB Encrypt Secrets Manager Ensure DevSecOps standard – data is encrypted using KMS Step 1 – Access AWS Key Management Service (KMS) Log in to AWS Console Search for service: KMS Select Key Management Service Go to menu Customer managed keys Click Create a key Step 2 – Configure Key At Key type, select Symmetric At Key usage, select Encrypt and decrypt Keep other options as default Click Next Step 3 – Add Labels (Alias \u0026amp; Description) Alias Enter:\ntaskhub_kms Click Next\nStep 4 – Define Key Administrative Permissions In the Key administrators list, select QuocBao Keep the option Allow key administrators to delete this key enabled Click Next The selected user at this step has full administrative permissions for the KMS Key:\nEdit key policy Enable / disable the key Delete the key Step 5 – Define Key Usage Permissions In the Key users list, select QuocBao No need to add Other AWS accounts Click Next Step 6 – Edit Key Policy At the Edit key policy step, click Edit Verify that the policy includes the following permission groups: Root account: kms:* User QuocBao is allowed to manage and use the key { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;key-consolepolicy-3\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Enable IAM User Permissions\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:root\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;kms:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow access for Key Administrators\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Create*\u0026#34;, \u0026#34;kms:Describe*\u0026#34;, \u0026#34;kms:Enable*\u0026#34;, \u0026#34;kms:List*\u0026#34;, \u0026#34;kms:Put*\u0026#34;, \u0026#34;kms:Update*\u0026#34;, \u0026#34;kms:Revoke*\u0026#34;, \u0026#34;kms:Disable*\u0026#34;, \u0026#34;kms:Get*\u0026#34;, \u0026#34;kms:Delete*\u0026#34;, \u0026#34;kms:TagResource\u0026#34;, \u0026#34;kms:UntagResource\u0026#34;, \u0026#34;kms:ScheduleKeyDeletion\u0026#34;, \u0026#34;kms:CancelKeyDeletion\u0026#34;, \u0026#34;kms:RotateKeyOnDemand\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow use of the key\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:ReEncrypt*\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;Allow attachment of persistent resources\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;AWS\u0026#34;: \u0026#34;arn:aws:iam::166557634525:user/QuocBao\u0026#34; }, \u0026#34;Action\u0026#34;: [ \u0026#34;kms:CreateGrant\u0026#34;, \u0026#34;kms:ListGrants\u0026#34;, \u0026#34;kms:RevokeGrant\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;Bool\u0026#34;: { \u0026#34;kms:GrantIsForAWSResource\u0026#34;: \u0026#34;true\u0026#34; } } } ] } Modify the policy if necessary to match your actual account ARN Click Next AWS will automatically attach a valid policy to the key.\nStep 7 – Review \u0026amp; Finish Review all configurations: Item Value Key type Symmetric Key usage Encrypt and decrypt Alias taskhub_kms Key Admin QuocBao Key User QuocBao Click Finish AWS will start creating the KMS Key.\nStep 8 – Verify KMS Key Created Successfully After creation, go back to KMS → Customer managed keys Verify the following information: Alias: taskhub_kms Status: Enabled Key type: Symmetric Key spec: SYMMETRIC_DEFAULT Key usage: Encrypt and decrypt The KMS Key has now been successfully created.\n"},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Revise and further develop the newly selected project topic Assign project tasks among team members Learn about DynamoDB Local and AWS security services Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Begin implementing the project 11/03/2025 11/03/2025 3 - Learn how to use and operate DynamoDB Local 11/04/2025 11/04/2025 https://000078.awsstudygroup.com/3-write-data-to-dynaomodb/ 4 - Continue project development 11/05/2025 11/05/2025 5 - Discuss project progress and explore security services such as KMS, Cognito, and WAF 11/06/2025 11/06/2025 6 - Continue project implementation 11/07/2025 11/07/2025 Week 9 Achievements: Finalized and refined the project topic, clearly understanding the scope and technical requirements. Assigned tasks within the team, ensuring clarity in roles and responsibilities. Gained knowledge on installing, operating, and integrating DynamoDB Local into the development workflow. Learned the roles of AWS security services such as KMS, Cognito, and WAF in building a secure system. "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Week 8 Objectives: Continue developing the project and the web interface Tasks to be implemented this week: Day Task Start Date End Date Reference Materials 2 - Project development 11/10/2025 11/10/2025 3 - Project development 11/11/2025 11/11/2025 4 - Project development 11/12/2025 11/12/2025 5 - Project development 11/13/2025 11/13/2025 6 - Project development 11/14/2025 11/14/2025 Results achieved in Week 10: Continued project implementation "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Objectives: Continue developing the project and the web interface Tasks to be implemented this week: Day Task Start Date End Date Reference Materials 2 - Project development 11/17/2025 11/17/2025 3 - Project development 11/18/2025 11/18/2025 4 - Project development 11/19/2025 11/19/2025 5 - Project development 11/20/2025 11/20/2025 6 - Project development 11/21/2025 11/21/2025 Results achieved in Week 11: Continued project implementation "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Objectives: Continue developing the project and deploy it on AWS services Tasks to be implemented this week: Day Task Start Date End Date Reference Materials 2 - Project development and deployment on AWS 10/27/2025 10/27/2025 3 - Project development and deployment on AWS 10/28/2025 10/28/2025 4 - Project development and deployment on AWS 10/29/2025 10/29/2025 5 - Project development and deployment on AWS 10/30/2025 10/30/2025 6 - Project development and deployment on AWS 10/31/2025 10/31/2025 Results achieved in Week 12: Continued project development and started migrating the project from local environment to the cloud "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.17-secretmanager/","title":"Configure AWS Secrets Manager","tags":[],"description":"","content":"Objective Use AWS Secrets Manager to store configuration/secrets for the TaskHub system with the following requirements:\nSecrets are stored in JSON format (Key/value pairs – Plaintext) Data is encrypted using KMS CMK: taskhub_kms (Optional) Enable automatic secret rotation using AWS Lambda Step 1 – Access AWS Secrets Manager In the AWS Console, type Secrets Manager in the search box. Select Secrets Manager from the Services list. Step 2 – Create a New Secret (Key/value pairs – JSON) On the Secrets Manager main page, click Store a new secret. In Secret type, select: Other type of secret.\nIn the Key/value pairs section:\nSwitch from the Key/value tab to the Plaintext tab. Paste the following JSON content (demo for DynamoDB + KMS): { \u0026#34;Service\u0026#34;: \u0026#34;Amazon DynamoDB\u0026#34;, \u0026#34;Table\u0026#34;: \u0026#34;TaskHub Tables\u0026#34;, \u0026#34;Encryption\u0026#34;: \u0026#34;SSE-KMS\u0026#34;, \u0026#34;KMSKeyAlias\u0026#34;: \u0026#34;taskhub_kms\u0026#34;, \u0026#34;Purpose\u0026#34;: \u0026#34;Store users, projects, tasks\u0026#34;, \u0026#34;DataProtection\u0026#34;: \u0026#34;Encrypted at rest\u0026#34; } In the Encryption key field, select the KMS key:\ntaskhub_kms Click Next.\nStep 3 – Configure Secret Name and Basic Information At the Configure secret step:\nSecret name:\nExample:\nprod/taskhub/secretmanager\nDescription (optional):\nMetadata for TaskHub DynamoDB encryption demo\nTags (optional): Skip for the workshop.\nResource permissions (optional): Keep default (IAM-based access control).\nReplicate secret (optional): Do not enable for this workshop.\nClick Next.\nStep 4 – Configure Automatic Rotation (Optional) In a production environment, secrets are usually rotated every 30 days.\nIn this workshop, a shorter interval is configured for demonstration purposes.\nAt Configure rotation – optional, enable: Automatic rotation In Rotation schedule: Select Schedule expression builder Time unit: Hours Hours: 23 (Optional) Window duration: 4h Keep Rotate immediately when the secret is stored checked a In Rotation function: Select the Lambda function: taskhub-backend Click Next. Step 5 – Review \u0026amp; Store the Secret On the Review step, verify the following information:\nSecret type: Other type of secret Encryption key: taskhub_kms Secret name: prod/taskhub/metadata Automatic rotation: Enabled Lambda rotation function: taskhub-backend Scroll down to the Sample code section:\nAWS provides built-in sample getSecret() functions for: Java JavaScript Python C# Go The TaskHub backend will use the corresponding SDK to retrieve secrets from AWS Secrets Manager instead of hard-coding them in source code. Click Store to complete the process.\nResult A new secret has been successfully created in AWS Secrets Manager. The secret content is stored in JSON format. The secret is: Encrypted at rest using KMS (taskhub_kms) Automatically rotatable using AWS Lambda The TaskHub backend can retrieve secrets via: AWS SDK IAM Role / Policy "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.18-waf/","title":"Set up Web ACL","tags":[],"description":"","content":"Step 1 – Access AWS WAF \u0026amp; Shield Sign in to the AWS Console Search for the service: WAF \u0026amp; Shield Click AWS WAF\nFrom the left menu, select:\nProtection packs (web ACLs)\nClick the button:\nCreate protection pack (web ACL)\nStep 2 – Select Application Type (Tell us about your app) 2.1 Select App Category Choose:\nAPI \u0026amp; integration services\nStep 3 – Select the CloudFront Distribution to Protect Expand Select resources to protect Click Add resources Select:\nGlobal → Add CloudFront or Amplify resources Check the CloudFront distribution of TaskHub (S3 frontend) Click Add Step 4 – Select the Rule Pack Type Select: ✅ Build your own pack from all of the protections AWS WAF offers\nIn the right panel, select: ✅ AWS-managed rule group\nClick Next Step 5 – Add Amazon IP Reputation List Select the rule: In Rule overrides, configure as follows: Rule Action AWSManagedIPReputationList ✅ Block AWSManagedReconnaissanceList ✅ Block AWSManagedIPDDoSList ✅ Count Click Add rule ✅ After this step, the rule will appear in the Add rules list.\nStep 6 – Verify the Added Rule Verify that the following information appears:\nRule: AWSManagedRulesAmazonIpReputationList Status: Saved WCU: 25 WCU Step 7 – Set the Web ACL Name In the Name and describe section:\nName: taskhub-waf Description: (leave blank or enter any description) Step 8 – Create the Protection Pack (Web ACL) Click: Create protection pack (web ACL)\nWait for AWS to complete the Web ACL creation "},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/5-workshop/5.3-lambda/","title":"","tags":[],"description":"","content":""},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://nguyenquocbaoily.github.io/fcj-workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]